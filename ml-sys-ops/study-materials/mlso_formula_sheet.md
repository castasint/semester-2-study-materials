# üìã ML System Optimization - Formula Sheet & Quick Reference

> **AIMLCZG516 | Midterm | Sessions 1-9**

---

## üéØ SPEEDUP & PERFORMANCE

### Speedup Definition
```
Speedup(p) = T_sequential / T_parallel(p)
```

### Amdahl's Law ‚≠ê MOST IMPORTANT
```
                    1
Speedup(p) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
               f + (1-f)/p

Where:
  f = serial fraction (NOT parallelizable)
  p = number of processors
  (1-f) = parallel fraction
```

### Maximum Speedup
```
Speedup_max = 1/f   (when p ‚Üí ‚àû)
```

### Efficiency
```
Efficiency(p) = Speedup(p) / p

Ideal efficiency = 1.0 (100%)
Linear speedup: Speedup = p
```

---

## üîÑ PARALLELISM TYPES

| Type | Description | Example |
|------|-------------|---------|
| **Data Parallelism** | Same op on different data | Vector addition |
| **Task Parallelism** | Different ops concurrently | Pipeline stages |
| **Request Parallelism** | Independent requests | Web server |
| **SPMD** | Single Program Multiple Data | Most ML |

---

## üßÆ k-MEANS CLUSTERING

### Algorithm Steps
```
1. Initialize k centers randomly
2. ASSIGN: Each point ‚Üí nearest center
3. UPDATE: New center = mean of cluster
4. REPEAT until convergence
```

### Complexity
```
Sequential: O(n √ó k √ó d √ó I)
  n = data points
  k = clusters
  d = dimensions
  I = iterations

Parallel (p processors): O(n/p √ó k √ó d √ó I)
Speedup ‚âà p (nearly linear)
```

---

## üìä MAPREDUCE

### Functions
```python
MAP:    (key, value) ‚Üí list of (key', value')
REDUCE: (key', list of values) ‚Üí (key', aggregated_value)
```

### Common Patterns
| Pattern | Map Output | Reduce |
|---------|------------|--------|
| Count | (item, 1) | sum |
| Sum | (key, value) | sum |
| Average | (key, (value, 1)) | sum/count |
| Max/Min | (key, value) | max/min |

---

## üñ•Ô∏è MEMORY MODELS

### Shared Memory
```
‚úì Fast communication (nanoseconds)
‚úì Easy programming (threads)
‚úó Limited scalability
Example: Multi-core CPU, GPU
```

### Distributed Memory
```
‚úì High scalability
‚úó Slow communication (milliseconds)
‚úó Complex programming (messages)
Example: Clusters, Cloud
```

---

## üåê PARAMETER SERVER

### Architecture
```
Server: Stores global model parameters w
Workers: Store local data, compute gradients

Each iteration:
  1. Workers PULL current w
  2. Workers compute gradients ‚àáL
  3. Workers PUSH gradients to server
  4. Server updates: w ‚Üê w - Œ∑ √ó avg(‚àáL)
```

### SGD Update Rule
```
w ‚Üê w - Œ∑ √ó (1/|B|) √ó Œ£ ‚àáL(x·µ¢, y·µ¢, w)

Œ∑ = learning rate
B = mini-batch
```

---

## üå≥ DECISION TREES

### Information Gain
```
IG(S, F) = H(S) - Œ£ (|S·µ•|/|S|) √ó H(S·µ•)
```

### Entropy
```
H(S) = -Œ£ p·µ¢ √ó log‚ÇÇ(p·µ¢)
```

### Parallelization
```
Each branch ‚Üí separate task
#parallel_tasks = #values of feature
```

---

## üíæ CACHE LOCALITY

### Types
```
TEMPORAL: Same location accessed repeatedly
SPATIAL: Nearby locations accessed together
```

### Matrix Multiplication
```
IJK order: Poor cache use for B (column access)
IKJ order: Good cache use (row access)
Speedup: up to 7x for large matrices!
```

### Access Time (Approximate)
```
L1 Cache:    ~4 cycles
L2 Cache:    ~12 cycles
L3 Cache:    ~40 cycles
RAM:         ~200 cycles
SSD:         ~50,000 cycles
HDD:         ~10,000,000 cycles
```

---

## üìê QUICK CALCULATIONS

### Amdahl's Law Examples
```
f = 0:    Speedup(p) = p (ideal)
f = 0.1:  Speedup(‚àû) = 10
f = 0.2:  Speedup(‚àû) = 5
f = 0.5:  Speedup(‚àû) = 2
```

### Speedup Table
```
f = 0.2 (20% serial):
  p=2:  Speedup = 1.67
  p=4:  Speedup = 2.50
  p=8:  Speedup = 3.33
  p=16: Speedup = 4.00
  p=‚àû:  Speedup = 5.00
```

---

## üîë KEY TERMS

| Term | Meaning |
|------|---------|
| **SPMD** | Single Program Multiple Data |
| **MPI** | Message Passing Interface |
| **SIMD** | Single Instruction Multiple Data |
| **GPGPU** | General Purpose GPU computing |
| **Throughput** | Operations per second |
| **Latency** | Time for single operation |
| **Scalability** | Performance scales with resources |

---

## ‚ö†Ô∏è COMMON EXAM TRAPS

1. **Speedup > p is impossible** (for most cases)
2. **f = 0 means ideal parallelization** (not realistic)
3. **Communication overhead** reduces speedup in distributed systems
4. **Synchronization** can serialize parallel code
5. **Load imbalance** means some processors wait
6. **Row-major vs Column-major** matters for cache performance

---

**Good luck! üçÄ**
