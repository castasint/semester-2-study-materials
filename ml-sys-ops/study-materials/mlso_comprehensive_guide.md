# ğŸ“š ML System Optimization - Comprehensive Study Guide

> **AIMLCZG516 | Midterm Prep | Sessions 1-9**

---

# Table of Contents

1. [Module 1: Introduction & Fundamentals](#module-1-introduction--fundamentals)
   - Session 1: ML Performance Metrics
   - Session 2: Parallel Programming Models & Speedup
   - Session 3: Modern Systems (Multicore, GPU, Clusters)
2. [Module 2: Parallel/Distributed ML Algorithms](#module-2-paralleldistributed-ml-algorithms)
   - Session 4: Task Parallelism
   - Session 5: k-Means Parallelization
   - Session 6: Review & MapReduce
   - Session 7: kNN & Decision Trees
3. [Module 3: Scale-out ML Systems](#module-3-scale-out-ml-systems)
   - Session 8: Parameter Server & Distributed SGD
   - Session 9: Neural Network Optimization & Locality

---

# Module 1: Introduction & Fundamentals

## Session 1: ML and Performance Metrics

### 1.1 Performance Metrics

| Metric | Definition | Formula |
|--------|------------|---------|
| **Time Complexity** | Algorithmic complexity | O(n), O(nÂ²), O(n log n) |
| **Running Time** | Actual wall-clock time | Measured in seconds |
| **Throughput** | Operations per unit time | ops/second |
| **Response Time** | Time to complete one request | Latency in ms |
| **Memory Usage** | RAM/Storage required | Bytes (MB, GB) |

### 1.2 Training vs Deployment Environments

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML SYSTEM LIFECYCLE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   TRAINING PHASE                    DEPLOYMENT PHASE         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚   â€¢ Large datasets (TB-PB)          â€¢ Small input (per req)  â”‚
â”‚   â€¢ High compute (GPU clusters)     â€¢ Low latency required   â”‚
â”‚   â€¢ Batch processing                â€¢ Real-time inference    â”‚
â”‚   â€¢ Accuracy-focused                â€¢ Throughput-focused     â”‚
â”‚   â€¢ Cloud/Data center               â€¢ Edge/Mobile/Cloud      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Range of Systems

| System Type | Characteristics | Use Case |
|-------------|-----------------|----------|
| **Cloud/Data Center** | Massive scale, distributed | Training large models |
| **GPU Clusters** | High parallelism, SIMD | Deep learning training |
| **Multi-core CPU** | Shared memory, threads | General ML workloads |
| **Embedded/Mobile** | Resource constrained | Edge inference |
| **TinyML Devices** | Ultra-low power | IoT, sensors |

---

## Session 2: Parallel Programming Models & Speedup

### 2.1 Types of Parallelism

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PARALLELISM TYPES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. DATA PARALLELISM (SPMD)                                  â”‚
â”‚     â”œâ”€â”€ Same operation on different data chunks              â”‚
â”‚     â”œâ”€â”€ E.g., Vector addition: A[i] + B[i] on each core     â”‚
â”‚     â””â”€â”€ Preferred when feasible (easy, efficient)            â”‚
â”‚                                                              â”‚
â”‚  2. TASK PARALLELISM (Pipeline)                              â”‚
â”‚     â”œâ”€â”€ Different operations run concurrently                â”‚
â”‚     â”œâ”€â”€ E.g., Stage1 â†’ Stage2 â†’ Stage3                      â”‚
â”‚     â””â”€â”€ Dependencies between stages                          â”‚
â”‚                                                              â”‚
â”‚  3. REQUEST PARALLELISM                                      â”‚
â”‚     â”œâ”€â”€ Independent requests processed in parallel           â”‚
â”‚     â”œâ”€â”€ E.g., Web server handling multiple requests          â”‚
â”‚     â””â”€â”€ No communication between requests                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 SPMD (Single Program Multiple Data)

- **Same program** executes on all processors
- Each processor works on **different data subset**
- Most common paradigm in ML parallelization

```python
# SPMD Example: Parallel Vector Addition
# Each processor p executes:
for i in range(start_p, end_p):
    C[i] = A[i] + B[i]
```

### 2.3 Speedup & Amdahl's Law â­ KEY CONCEPT

**Speedup Definition:**
$$\text{Speedup}(p) = \frac{T_{\text{sequential}}}{T_{\text{parallel}}(p)}$$

**Amdahl's Law:**
```
If fraction f of a program is NOT parallelizable:

                    1
    Speedup(p) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  f + (1-f)/p

Where:
  f = serial fraction (cannot be parallelized)
  p = number of processors
  (1-f) = parallel fraction
```

**Key Insights:**
- If f = 0 (perfectly parallel): Speedup = p (ideal/linear)
- If f = 0.1 (10% serial): Max speedup â‰ˆ 10 (even with âˆ processors)
- If f = 0.5 (50% serial): Max speedup = 2

### 2.4 Speedup Example Calculation

**Problem:** A program takes 100 seconds. 20% is inherently sequential.

```
Given: f = 0.2, T_seq = 100s

For p = 4 processors:
  Speedup(4) = 1 / (0.2 + 0.8/4) = 1 / (0.2 + 0.2) = 1/0.4 = 2.5
  T_parallel = 100 / 2.5 = 40 seconds

For p = 10 processors:
  Speedup(10) = 1 / (0.2 + 0.8/10) = 1 / (0.2 + 0.08) = 1/0.28 = 3.57
  T_parallel = 100 / 3.57 = 28 seconds

Maximum Speedup (p â†’ âˆ):
  Speedup_max = 1 / f = 1 / 0.2 = 5
```

### 2.5 Factors Limiting Parallelism

| Factor | Description | Impact |
|--------|-------------|--------|
| **Memory Contention** | Multiple processors accessing same memory | Serialization |
| **Data Dependencies** | Output of one task needed as input to another | Forced ordering |
| **Synchronization** | Waiting for other processors | Idle time |
| **Communication Overhead** | Data transfer between processors | Extra time |
| **Load Imbalance** | Uneven work distribution | Some processors idle |

---

## Session 3: Modern Systems Architecture

### 3.1 Shared Memory vs Distributed Memory

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SHARED MEMORY MODEL                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚        â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”                       â”‚
â”‚        â”‚ P1 â”‚  â”‚ P2 â”‚  â”‚ P3 â”‚  â”‚ Pp â”‚                       â”‚
â”‚        â””â”€â”€â”¬â”€â”˜  â””â”€â”€â”¬â”€â”˜  â””â”€â”€â”¬â”€â”˜  â””â”€â”€â”¬â”€â”˜                       â”‚
â”‚           â”‚      â”‚      â”‚      â”‚                            â”‚
â”‚        â•â•â•â•§â•â•â•â•â•â•â•§â•â•â•â•â•â•â•§â•â•â•â•â•â•â•§â•â•â•                         â”‚
â”‚        â”‚     INTERCONNECTION BUS    â”‚                        â”‚
â”‚        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                         â”‚
â”‚                     â”‚                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚              â”‚   GLOBAL    â”‚                                â”‚
â”‚              â”‚   MEMORY    â”‚                                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                              â”‚
â”‚  â€¢ Multi-core CPUs, GPUs                                    â”‚
â”‚  â€¢ Fast communication (ns)                                  â”‚
â”‚  â€¢ Limited scalability                                      â”‚
â”‚  â€¢ Programming: Threads (OpenMP, pthreads)                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            DISTRIBUTED MEMORY MODEL                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Memory  â”‚   â”‚ Memory  â”‚   â”‚ Memory  â”‚   â”‚ Memory  â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚   P1    â”‚   â”‚   P2    â”‚   â”‚   P3    â”‚   â”‚   Pp    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚            â”‚            â”‚            â”‚              â”‚
â”‚  â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•     â”‚
â”‚  â”‚           COMMUNICATION NETWORK              â”‚           â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•            â”‚
â”‚                                                              â”‚
â”‚  â€¢ Clusters, Cloud                                          â”‚
â”‚  â€¢ Slower communication (ms)                                â”‚
â”‚  â€¢ High scalability                                         â”‚
â”‚  â€¢ Programming: MPI, Spark, MapReduce                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Memory Hierarchy

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  REGISTERS   â”‚  â† Fastest (1 cycle)
                    â”‚   (~1 KB)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   L1 CACHE   â”‚  â† ~4 cycles
                    â”‚  (~32 KB)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   L2 CACHE   â”‚  â† ~12 cycles
                    â”‚  (~256 KB)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   L3 CACHE   â”‚  â† ~40 cycles
                    â”‚   (~8 MB)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     RAM      â”‚  â† ~200 cycles
                    â”‚  (~16 GB)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     SSD      â”‚  â† ~50,000 cycles
                    â”‚  (~512 GB)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     HDD      â”‚  â† ~10,000,000 cycles
                    â”‚  (~2 TB)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Module 2: Parallel/Distributed ML Algorithms

## Session 4: Task Parallelism & Problem Decomposition

### 4.1 Task Parallelism

Unlike data parallelism where same operation runs on different data, **task parallelism** runs different operations concurrently.

```
Example: Pipeline Parallelism

  Input â†’ [Task A] â†’ [Task B] â†’ [Task C] â†’ Output
              â†“          â†“          â†“
           Stage 1    Stage 2    Stage 3
           
  While Task A processes item 2,
  Task B can process item 1 (already done by A)
```

### 4.2 Problem Decomposition Strategies

| Strategy | Description | Best For |
|----------|-------------|----------|
| **Domain Decomposition** | Split data among processors | Large datasets |
| **Functional Decomposition** | Split computation into stages | Pipeline workloads |
| **Recursive Decomposition** | Divide-and-conquer | Tree structures |

---

## Session 5: k-Means Parallelization â­ KEY ALGORITHM

### 5.1 k-Means Algorithm (Sequential)

```
Algorithm: k-Means Clustering
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input: Dataset D, number of clusters k
Output: k cluster centers câ‚, câ‚‚, ..., câ‚–

1. Initialize: Choose k random points as initial centers
2. REPEAT:
   a. ASSIGN: For each point x in D:
      - Compute distance to all k centers
      - Assign x to nearest cluster
   b. UPDATE: For each cluster j:
      - câ±¼ = mean of all points assigned to cluster j
3. UNTIL: Centers converge (don't change)
```

### 5.2 k-Means Using MapReduce

```
Step 1: Initialize k centers (random selection)

Step 2 (Assign): MAP operation
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MAP: For each point xáµ¢                      â”‚
  â”‚   - Compute distances to all k centers      â”‚
  â”‚   - Output: (cluster_id, xáµ¢)               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  REDUCE: min over distances
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ REDUCE: For each point                      â”‚
  â”‚   - Find cluster with minimum distance      â”‚
  â”‚   - Assign point to that cluster            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 3 (Update): MAP + REDUCE
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MAP: For each cluster j                     â”‚
  â”‚   - Output: (j, point)                      â”‚
  â”‚                                             â”‚
  â”‚ REDUCE: For cluster j                       â”‚
  â”‚   - câ±¼ = (sum of all points) / count       â”‚
  â”‚     i.e., compute centroid                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 4: Check convergence, repeat if needed
```

### 5.3 k-Means Speedup Analysis

```
Sequential Time Complexity (per iteration):
  T_seq = |D| Ã— (k + k) + k Ã— |C|
        = 2k|D| + k|C|
        
  Where:
    |D| = number of data points
    k = number of clusters
    |C| = average cluster size

Parallel Time (p processors):
  T_par = |D|/p Ã— 2k + |C|
  
Speedup:
  S(p) = T_seq / T_par â‰ˆ p (nearly linear!)
```

### 5.4 k-Means Code (Spark-style)

```python
# Pseudocode for parallel k-Means

def parallel_kmeans(data, k, max_iters):
    # Initialize centers randomly
    centers = data.takeSample(k)
    
    for iteration in range(max_iters):
        # ASSIGN: Map each point to nearest center
        assignments = data.map(
            lambda x: (find_nearest_center(x, centers), x)
        )
        
        # UPDATE: Compute new centers
        new_centers = assignments \
            .mapValues(lambda x: (x, 1)) \
            .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \
            .mapValues(lambda v: v[0] / v[1]) \
            .collect()
        
        # Check convergence
        if converged(centers, new_centers):
            break
        centers = new_centers
    
    return centers
```

---

## Session 6: Review & MapReduce

### 6.1 MapReduce Programming Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MAPREDUCE FLOW                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  INPUT     MAP PHASE      SHUFFLE/SORT     REDUCE PHASE     â”‚
â”‚  DATA                                                        â”‚
â”‚                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚Doc1 â”‚â†’ â”‚Mapper 1â”‚â†’ (k1,v1)              â”‚        â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â†˜                  â”‚        â”‚        â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”  (k1,*) â”‚Reducer1â”‚â†’Out1   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚      â”‚  â”€â”€â”€â”€â†’  â”‚        â”‚        â”‚
â”‚ â”‚Doc2 â”‚â†’ â”‚Mapper 2â”‚â†’ (k2,v2)â”€â”€â”€â”€â”‚ SORT â”‚         â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â†—  â”‚  BY  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                          â”‚ KEY  â”‚                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚      â”‚  (k2,*) â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚Doc3 â”‚â†’ â”‚Mapper 3â”‚â†’ (k1,v3)â”€â”€â”€â”€â”‚      â”‚  â”€â”€â”€â”€â†’  â”‚Reducer2â”‚â†’Out2   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜         â”‚        â”‚        â”‚
â”‚                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 MapReduce Functions

```python
# MAP: Transform input to key-value pairs
def map(key, value):
    # Process one input record
    # Emit zero or more (key, value) pairs
    emit(new_key, new_value)

# REDUCE: Aggregate values for each key
def reduce(key, values[]):
    # Process all values for one key
    result = aggregate(values)
    emit(key, result)
```

### 6.3 Common MapReduce Patterns

| Pattern | Example | Map Output | Reduce Op |
|---------|---------|------------|-----------|
| **Word Count** | Count words in docs | (word, 1) | sum |
| **Summation** | Total sales | (category, amount) | sum |
| **Average** | Avg temperature | (city, (temp, 1)) | sum/count |
| **Max/Min** | Highest score | (user, score) | max |
| **Filter** | Find matches | (match, record) | identity |

---

## Session 7: kNN & Decision Trees Parallelization

### 7.1 kNN (k-Nearest Neighbors Parallelization

```
Sequential kNN:
  For each query point q:
    1. Compute distance to ALL training points
    2. Find k nearest neighbors
    3. Vote on class label

Parallel kNN (Data Parallel):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Partition training data among p processors    â”‚
  â”‚                                               â”‚
  â”‚ Each processor:                               â”‚
  â”‚   - Compute distances to local data          â”‚
  â”‚   - Find LOCAL k nearest neighbors           â”‚
  â”‚                                               â”‚
  â”‚ REDUCE:                                       â”‚
  â”‚   - Merge all local k-nearest lists          â”‚
  â”‚   - Select GLOBAL k nearest                  â”‚
  â”‚   - Vote for final prediction                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Decision Trees Parallelization

```
Decision Tree Construction (ID3 Algorithm):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. If all examples have same label â†’ Return leaf
2. If no features left â†’ Return leaf with majority label
3. Choose feature F with max Information Gain
4. For each value v of F:
   - Create branch
   - Recursively build subtree for subset with F=v

Parallelization Strategy (Task Parallelism):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ At each level:                                â”‚
  â”‚   - Each branch can be built INDEPENDENTLY    â”‚
  â”‚   - Assign different branches to processors   â”‚
  â”‚                                               â”‚
  â”‚ Number of parallel tasks = number of values   â”‚
  â”‚ of the chosen feature                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 Information Gain

$$\text{IG}(S, F) = H(S) - \sum_{v \in Values(F)} \frac{|S_v|}{|S|} H(S_v)$$

Where:
- H(S) = Entropy of set S
- F = Feature being tested
- Sáµ¥ = Subset of S where F = v

$$H(S) = -\sum_{c} p_c \log_2(p_c)$$

---

# Module 3: Scale-out ML Systems

## Session 8: Parameter Server & Distributed SGD â­ KEY CONCEPT

### 8.1 Distributed ML Challenges

```
Training Data Size: 1TB to 1PB
Model Parameters: 10â¹ to 10Â¹Â² parameters

Examples:
  - Online Recommender: Millions of user profiles
  - Ad Click Predictor: High-dimensional feature vectors
```

### 8.2 Parameter Server Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PARAMETER SERVER MODEL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                 â”‚  PARAMETER SERVER â”‚                       â”‚
â”‚                 â”‚  (stores w)       â”‚                       â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                           â”‚                                  â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚            â”‚              â”‚              â”‚                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”           â”‚
â”‚      â”‚  WORKER   â”‚  â”‚  WORKER   â”‚  â”‚  WORKER   â”‚           â”‚
â”‚      â”‚    W0     â”‚  â”‚    W1     â”‚  â”‚    Wp     â”‚           â”‚
â”‚      â”‚  Data D0  â”‚  â”‚  Data D1  â”‚  â”‚  Data Dp  â”‚           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                              â”‚
â”‚  Each iteration:                                            â”‚
â”‚    1. Workers PULL current w from server                    â”‚
â”‚    2. Workers compute local gradients on their data         â”‚
â”‚    3. Workers PUSH gradients to server                      â”‚
â”‚    4. Server aggregates and updates w                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.3 ML as Regularized Error Minimization

```
Training objective function:

  F(w) = Î£áµ¢ L(xáµ¢, yáµ¢, w) + Î»R(w)

Where:
  w = model parameters (weights)
  L = loss function (prediction error)
  R = regularizer (penalizes complexity)
  Î» = regularization strength
```

### 8.4 Stochastic Gradient Descent (SGD)

```
Batch Gradient Descent:
  w â† w - Î· Â· âˆ‡F(w)
  
  Problem: Computing gradient over ALL data is expensive

Stochastic Gradient Descent:
  For each mini-batch B:
    w â† w - Î· Â· (1/|B|) Î£áµ¢âˆˆB âˆ‡L(xáµ¢, yáµ¢, w)
    
  Advantage: Updates after each mini-batch
             Better for large datasets
```

### 8.5 Distributed SGD

```
Synchronous SGD:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1. All workers compute gradients in parallel   â”‚
  â”‚ 2. BARRIER: Wait for all workers               â”‚
  â”‚ 3. Aggregate gradients (average)               â”‚
  â”‚ 4. Update model                                â”‚
  â”‚ 5. Repeat                                      â”‚
  â”‚                                                â”‚
  â”‚ âœ“ Consistent                                   â”‚
  â”‚ âœ— Slow (waits for stragglers)                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Asynchronous SGD:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1. Worker computes gradient                    â”‚
  â”‚ 2. Worker sends update immediately             â”‚
  â”‚ 3. Server applies update                       â”‚
  â”‚ 4. Worker pulls new model, continues           â”‚
  â”‚                                                â”‚
  â”‚ âœ“ Fast (no waiting)                           â”‚
  â”‚ âœ— Stale gradients (may hurt convergence)      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Session 9: Neural Network Optimization & Locality

### 9.1 Locality of Reference

```
Two types of locality observed in programs:

TEMPORAL LOCALITY:
  - Recently accessed locations will be accessed again
  - E.g., loop variables, frequently used data
  
SPATIAL LOCALITY:
  - Locations near recently accessed will be accessed
  - E.g., array elements, sequential instructions
```

### 9.2 Memory Hierarchy Optimization

```
System designers exploit locality:

CACHING (temporal locality):
  - Keep recently accessed data close to processor
  
PRE-FETCHING (spatial locality):
  - Load nearby data before it's needed
  
BLOCKING/BUFFERING:
  - Access data in large chunks
  - Amortizes setup costs (disk seek, network latency)
```

### 9.3 Matrix Multiplication Optimization â­ KEY EXAMPLE

**Naive Algorithm (IJK order):**
```c
// Poor cache performance
for (i = 0; i < n; i++)
    for (j = 0; j < n; j++)
        for (k = 0; k < n; k++)
            c[i][j] += a[i][k] * b[k][j];

// Access pattern:
// a[i][*] accessed row-wise (good)
// b[*][j] accessed column-wise (BAD - cache misses!)
```

**Cache-Aware Algorithm (IKJ order):**
```c
// Better cache performance
for (i = 0; i < n; i++) {
    for (j = 0; j < n; j++) c[i][j] = 0;
    for (k = 0; k < n; k++)
        for (j = 0; j < n; j++)
            c[i][j] += a[i][k] * b[k][j];
}

// Access pattern:
// a[i][k] accessed once per outer iteration (good)
// b[k][*] accessed row-wise (GOOD!)
// c[i][*] cached and reused (BEST!)
```

### 9.4 Performance Comparison (from Lecture 9)

| Method | n=256 | n=512 | n=1024 | n=2048 | n=4096 |
|--------|-------|-------|--------|--------|--------|
| **IJK** | 0.11s | 0.93s | 10.41s | ~450s | ~4026s |
| **IKJ** | 0.14s | 1.12s | 8.98s | ~73s | ~581s |
| **Speedup** | 0.80 | 0.83 | 1.16 | **6.19** | **6.93** |

**Key insight:** Same algorithm, different loop order â†’ **7x speedup** for large matrices!

---

# ğŸ“‹ Quick Reference Formulas

## Speedup & Amdahl's Law
```
Speedup(p) = T_seq / T_par(p)

Amdahl's Law: Speedup(p) = 1 / (f + (1-f)/p)
  where f = serial fraction, p = processors

Max Speedup = 1/f  (when p â†’ âˆ)
```

## Efficiency
```
Efficiency(p) = Speedup(p) / p

Ideal efficiency = 1 (100%)
```

## k-Means Complexity
```
Per iteration: O(n Ã— k Ã— d)
  where n = data points, k = clusters, d = dimensions
```

## Information Gain
```
IG(S, F) = H(S) - Î£áµ¥ (|Sáµ¥|/|S|) Ã— H(Sáµ¥)

Entropy: H(S) = -Î£ páµ¢ logâ‚‚(páµ¢)
```

## SGD Update
```
w â† w - Î· Ã— âˆ‡L(w)
  where Î· = learning rate
```

---

# ğŸ“ Practice Problems

## Problem 1: Amdahl's Law

A program runs in 200 seconds. 30% of it is inherently sequential.

a) What is the maximum possible speedup?
b) What speedup can be achieved with 8 processors?
c) How many processors are needed to achieve 2.5x speedup?

**Solution:**
```
Given: T_seq = 200s, f = 0.3

a) Max Speedup = 1/f = 1/0.3 = 3.33

b) Speedup(8) = 1/(0.3 + 0.7/8) = 1/(0.3 + 0.0875) = 1/0.3875 = 2.58

c) 2.5 = 1/(0.3 + 0.7/p)
   0.3 + 0.7/p = 0.4
   0.7/p = 0.1
   p = 7 processors
```

## Problem 2: k-Means Speedup

You have 10,000 data points, 5 clusters. Sequential time = 50ms per iteration.

a) If you use 10 processors, estimate the parallel time (assume ideal speedup).
b) What communication overhead would reduce speedup to only 5x?

**Solution:**
```
a) Ideal speedup = 10
   T_par = 50ms / 10 = 5ms per iteration

b) If actual speedup = 5:
   T_par = 50/5 = 10ms
   Computation time = 5ms
   Communication overhead = 10 - 5 = 5ms
```

## Problem 3: MapReduce Word Count

Write map and reduce functions for counting word frequency.

**Solution:**
```python
def map(doc_id, doc_text):
    for word in doc_text.split():
        emit(word, 1)

def reduce(word, counts):
    emit(word, sum(counts))
```

## Problem 4: Matrix Multiplication Cache Analysis

For nÃ—n matrix multiplication, the naive IJK algorithm has O(nÂ³) memory accesses.

a) How many cache misses for B matrix in IJK order (assuming cache holds one row)?
b) How many cache misses for B matrix in IKJ order?

**Solution:**
```
a) IJK order: B[k][j] accessed column-wise
   - Each B[k][j] access is a cache miss (different rows)
   - Total misses for B: O(nÂ³)

b) IKJ order: B[k][j] accessed row-wise
   - Row B[k][*] loaded once per (i,k) iteration
   - Total misses for B: O(nÂ²)
   
   Improvement: O(n) reduction in cache misses!
```

---

# ğŸ“º Recommended Resources

## NPTEL Lectures
- Parallel Computing (IIT Kanpur)
- Distributed Systems (IIT Kharagpur)
- High Performance Computing (IIT Madras)

## Topics to Review
1. Amdahl's Law and speedup calculations
2. MapReduce programming model
3. k-Means parallelization
4. Parameter Server architecture
5. Cache locality and matrix multiplication
6. Shared vs Distributed memory models

---

**Good luck with your exam! ğŸ€**
