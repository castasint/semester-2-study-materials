# ğŸ“‹ DNN Midterm Formula Sheet
### Quick Reference Card | AIMLCZG511 | Sessions 1-8

---

## ğŸ§  PERCEPTRON

### Weighted Sum
```
z = wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ = wáµ€x
```

### Activation (Step/Sign)
```
Å· = sign(z) = { +1 if z â‰¥ 0
              { -1 if z < 0
```

### Weight Update Rule
```
Î”wáµ¢ = Î·(t - Å·)xáµ¢
wáµ¢ â† wáµ¢ + Î”wáµ¢
```

### XOR Perceptrons
- Single hidden layer: n perceptrons (n = inputs)
- Deep network: O(log n) perceptrons

---

## ğŸ“ˆ LINEAR REGRESSION

### Model
```
Å· = wáµ€x = wâ‚€ + wâ‚xâ‚ + ... + wâ‚xâ‚
Å· = Xw  (vectorized)
```

### MSE Loss
```
J(w) = (1/2N) Î£(Å·â½â±â¾ - yâ½â±â¾)Â²
     = (1/2N) ||Xw - y||Â²
```

### Gradient
```
âˆ‡J = (1/N) Xáµ€(Xw - y)
```

### Update Rule
```
w â† w - Î·âˆ‡J
```

---

## ğŸ¯ BINARY CLASSIFICATION

### Sigmoid
```
Ïƒ(z) = 1 / (1 + eâ»á¶»)
Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
```

### Prediction
```
Å· = Ïƒ(wáµ€x) = P(y=1|x)
class = 1 if Å· â‰¥ 0.5 else 0
```

### Binary Cross-Entropy (BCE)
```
â„“ = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
J = -(1/N) Î£[yâ½â±â¾log(Å·â½â±â¾) + (1-yâ½â±â¾)log(1-Å·â½â±â¾)]
```

### Gradient (same form!)
```
âˆ‡â„“ = (Å· - y)x
```

---

## ğŸ¨ MULTI-CLASS CLASSIFICATION

### Softmax
```
Å·â‚– = exp(zâ‚–) / Î£â±¼exp(zâ±¼)
```

### Categorical Cross-Entropy (CCE)
```
â„“ = -Î£â‚– yâ‚–Â·log(Å·â‚–)
```
(y is one-hot encoded)

---

## ğŸ”¥ ACTIVATION FUNCTIONS

| Function | Formula | Derivative | Range |
|----------|---------|------------|-------|
| **Sigmoid** | 1/(1+eâ»á¶») | Ïƒ(1-Ïƒ) | (0,1) |
| **Tanh** | (eá¶»-eâ»á¶»)/(eá¶»+eâ»á¶») | 1-tanhÂ²(z) | (-1,1) |
| **ReLU** | max(0,z) | 1 if z>0, else 0 | [0,âˆ) |
| **Softmax** | eá¶»â±/Î£eá¶»Ê² | - | (0,1) |
| **Identity** | z | 1 | (-âˆ,âˆ) |

---

## ğŸŒ DFNN FORWARD PROPAGATION

### Per Layer
```
zâ½â„“â¾ = hâ½â„“â»Â¹â¾Wâ½â„“â¾ + bâ½â„“â¾
hâ½â„“â¾ = Ïƒ(zâ½â„“â¾)
```

### Initial & Final
```
hâ½â°â¾ = x
Å· = hâ½á´¸â¾
```

---

## â¬…ï¸ BACKPROPAGATION

### Output Layer Error
```
Î´â½á´¸â¾ = (1/B)(Å¶ - Y)
```
(Works for MSE+Id, BCE+Sigmoid, CCE+Softmax)

### Hidden Layer Error
```
Î´â½â„“â¾ = (Î´â½â„“âºÂ¹â¾Wâ½â„“âºÂ¹â¾áµ€) âŠ™ Ïƒ'(zâ½â„“â¾)
```

### Gradients
```
âˆ‚J/âˆ‚Wâ½â„“â¾ = (1/B) Hâ½â„“â»Â¹â¾áµ€Î´â½â„“â¾
âˆ‚J/âˆ‚bâ½â„“â¾ = (1/B) 1áµ€Î´â½â„“â¾
```

---

## ğŸ“Š PARAMETER COUNT

### With Bias
```
Total = Î£ nâ‚—(nâ‚—â‚‹â‚ + 1)
```

### Without Bias
```
Total = Î£ nâ‚— Ã— nâ‚—â‚‹â‚
```

### Example: 784â†’256â†’128â†’10
```
Layer 1: 784Ã—256 + 256 = 200,960
Layer 2: 256Ã—128 + 128 = 32,896
Layer 3: 128Ã—10 + 10 = 1,290
Total: 235,146
```

---

## ğŸ–¼ï¸ CNN FORMULAS

### Output Size
```
output = âŒŠ(n + 2p - f) / sâŒ‹ + 1
```
Where:
- n = input size
- p = padding
- f = filter/kernel size
- s = stride

### Common Cases
| Padding | Formula | Name |
|---------|---------|------|
| p = 0 | (n-f)/s + 1 | Valid |
| p = (f-1)/2 | n (if s=1) | Same |

### Pooling Output
```
output = âŒŠ(n - pool_size) / strideâŒ‹ + 1
```

---

## ğŸ“ CLASSIFICATION METRICS

### Confusion Matrix
```
              Predicted
             Pos    Neg
Actual Pos   TP     FN
       Neg   FP     TN
```

### Formulas
```
Accuracy  = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)
Recall    = TP / (TP + FN)
F1        = 2 Ã— (P Ã— R) / (P + R)
```

---

## ğŸ—ï¸ CNN ARCHITECTURES

### LeNet (1998)
```
Conv(5Ã—5,6) â†’ Pool â†’ Conv(5Ã—5,16) â†’ Pool â†’ FC â†’ FC â†’ Out
```

### AlexNet (2012)
- Large kernels (11Ã—11, 5Ã—5)
- ReLU, Dropout
- GPU training

### VGG (2014)
- Only 3Ã—3 kernels
- Very deep (16-19 layers)
- 138M parameters

### ResNet (2015)
- Skip connections: output = F(x) + x
- 100+ layers possible
- Solves vanishing gradient

---

## ğŸ”„ GRADIENT DESCENT VARIANTS

| Type | Updates per Epoch | Use Case |
|------|-------------------|----------|
| **Batch GD** | 1 (all data) | Small dataset |
| **SGD** | N (per sample) | Large dataset |
| **Mini-batch** | N/B (batches) | Standard |

### Common Batch Sizes
32, 64, 128, 256

---

## ğŸ›ï¸ LOSS SUMMARY

| Task | Activation | Loss |
|------|------------|------|
| **Regression** | Identity | MSE |
| **Binary Class** | Sigmoid | BCE |
| **Multi-class** | Softmax | CCE |

---

## âš¡ QUICK TIPS

1. **Perceptron**: Only for linearly separable
2. **Sigmoid output**: For probabilities [0,1]
3. **ReLU hidden**: Avoids vanishing gradient
4. **Softmax**: Makes probabilities sum to 1
5. **BCE vs CCE**: Binary vs multi-class
6. **Transfer Learning**: Pre-trained â†’ new task
7. **Skip connections**: Enable deep networks

---

## ğŸ”¢ NUMERICAL TIPS

### Sigmoid Values
```
Ïƒ(0) = 0.5
Ïƒ(1) â‰ˆ 0.731
Ïƒ(2) â‰ˆ 0.881
Ïƒ(-1) â‰ˆ 0.269
Ïƒ(-2) â‰ˆ 0.119
```

### Common e Values
```
eâ° = 1
eÂ¹ â‰ˆ 2.718
eÂ² â‰ˆ 7.389
eÂ³ â‰ˆ 20.09
eâ»Â¹ â‰ˆ 0.368
eâ»Â² â‰ˆ 0.135
```

### log Values
```
log(0.5) â‰ˆ -0.693
log(0.1) â‰ˆ -2.303
log(0.9) â‰ˆ -0.105
```

---

**ğŸ€ Good luck with your exam!**
