# -*- coding: utf-8 -*-
"""CS_3_ML_Linear Regression Using Gradient Descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sh27kd9Ch1rr9a6mA2ImZ3BQbmohakRR

# Linear Regression using Gradient Descent

## Linear Regression by implementing of gradient descent algorithm

The linear regression line is defined as $$y = \theta_0 + \theta_1 x $$

The parameters $\theta_0$ and $\theta_1$ can be computed using gradeint descent algorithm.

Gradeint descent algorithm is given as:


repeat until convergence {
\begin{align*}
    \theta_1 &= \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^{m} \big ( h_\theta (x^{(i)}) - y^{(i)} \big ) * x^{(i)} \\
    \theta_0 &= \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m} \big ( h_\theta (x^{(i)}) - y^{(i)} \big )
\end{align*}
}

Reference: https://towardsdatascience.com/

### Step 1: Import libraries and dataset
"""

# Commented out IPython magic to ensure Python compatibility.
## Import all the necessary libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# %matplotlib inline

## Import the dataset

data = pd.read_csv('salary_data.csv')
# change the path as necessary

X = data.iloc[:, 0]
Y = data.iloc[:, 1]

# View the size of the arrays X and Y
print(X.shape)
print(Y.shape)

"""### Step 2: EDA , Preprocessing , Create the training data"""

# Visualize the dataset

plt.scatter(X, Y, color='blue')
plt.title('Salary VS Experience (Observations)')
plt.xlabel('Year of Experience')
plt.ylabel('Salary')
plt.show()

# Split the data into train vs test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

"""Step 2 is skipped.

### Step 3: Train the machine learning model
"""

# Build the model


theta1 = 0
theta0 = 0
#Y_pred = theta1 * X + theta0
alpha = 0.0001  # Learning Rate
epochs = 10000  # Number of iterations to perform gradient descent

m = float(len(X_train)) # Number of elements in X

cost_history = []

cost_onTestData_history = []

# Performing Gradient Descent
for i in range(epochs):
    Y_pred = theta1 * X_train + theta0
    Y_Test_pred = theta1 * X_test + theta0

    temp1 = (-1/m) * sum(X_train * (y_train - Y_pred))
    temp0 = (-1/m) * sum(y_train - Y_pred)
    theta1 = theta1 - alpha * temp1
    theta0 = theta0 - alpha * temp0

    costTrain = (1/2*m) * sum((y_train - Y_pred)**2)
    costTest = (1/2*m) * sum((y_test - Y_Test_pred)**2)

    cost_history.append(costTrain)
    cost_onTestData_history.append(costTest)

"""### Step 4: Visualize the results"""

# The coefficients

# print the parameter theta1
print('Theta1 = ', theta1)
# print the parameter theta0
print('Theta0 = ', theta0)

"""### Step 5: Prediction"""

# Predict the values for the given X
Y_pred = theta1 * X_train + theta0
Y_pred

# Visualize the dataset and plot the residuals

fig, ax = plt.subplots()

ax.scatter(X_train, y_train, color='blue')       # observed values
ax.scatter(X_train, Y_pred, color='green') # predicted values
ax.vlines(X_train,y_train, Y_pred, color='red')  # residual lines
plt.plot([min(X_train), max(X_train)], [min(Y_pred), max(Y_pred)], color='black')  # regression line

plt.title('Salary VS Experience')
plt.xlabel('Year of Experience')
plt.ylabel('Salary')
plt.show()

# plot the cost function

plt.plot(cost_history)
plt.plot(cost_onTestData_history, color="green")
plt.title('Cost Function using Gradient Descent')
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
plt.show()

"""### Step 6: Model Evalaution & Assessment using Performance measures"""

from sklearn.metrics import mean_squared_error, r2_score
print("Performance on Training Set")
# The mean squared error
print("Mean squared error = %.2f" % mean_squared_error(y_train, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score = %.2f' % r2_score(y_train, Y_pred))

# Predict the values for the test data
Y_Test_pred = theta1 * X_test + theta0
print("Performance on Test Set")
# The mean squared error
print("Mean squared error = %.2f" % mean_squared_error(y_test, Y_Test_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score = %.2f' % r2_score(y_test, Y_Test_pred))