# üöÄ DRL 5-Hour Crash Course

> **AIMLCZG512 | Exam: 4 Questions √ó 7.5% | Closed Book | 2 Hours**

---

## ‚è±Ô∏è TIME ALLOCATION

| Hour | Topic | Weight |
|------|-------|--------|
| **1** | Multi-Armed Bandits | ~25% |
| **2** | MDP & Bellman Equations | ~30% |
| **3** | Dynamic Programming | ~25% |
| **4** | Monte Carlo Methods | ~20% |
| **5** | Practice Problems | Review |

---

# HOUR 1: Multi-Armed Bandits

## üéØ Key Formula #1: Incremental Update

```
Q_{n+1} = Q_n + Œ± [R_n - Q_n]
        = Q_n + Œ± √ó Error
```

**Practice right now:**
```
Q = 3.0, R = 5.0, Œ± = 0.1
Q_new = 3.0 + 0.1(5.0 - 3.0) = 3.0 + 0.2 = 3.2
```

## üéØ Key Formula #2: Œµ-Greedy

```
|A| = number of actions
Best action:  P = 1 - Œµ + Œµ/|A|
Other action: P = Œµ/|A|
```

**Practice:**
```
Œµ = 0.2, |A| = 4
P(best) = 1 - 0.2 + 0.2/4 = 0.8 + 0.05 = 0.85
P(other) = 0.2/4 = 0.05
```

## üéØ Key Formula #3: UCB

```
A = argmax [ Q(a) + c‚àö(ln t / N(a)) ]
```

## ‚úÖ Hour 1 Checkpoint
- [ ] Can do incremental Q update
- [ ] Can calculate Œµ-greedy probabilities
- [ ] Know Œ± = 1/n vs constant Œ± difference

---

# HOUR 2: MDP & Bellman Equations

## üéØ MDP = (S, A, P, R, Œ≥)

```
S = States
A = Actions
P(s'|s,a) = Transition probability
R = Reward
Œ≥ = Discount factor
```

## üéØ Return Calculation (Work Backwards!)

```
Given rewards [r‚ÇÅ, r‚ÇÇ, r‚ÇÉ], Œ≥ = 0.9:

G‚ÇÉ = 0 (terminal)
G‚ÇÇ = r‚ÇÉ + Œ≥√óG‚ÇÉ = r‚ÇÉ
G‚ÇÅ = r‚ÇÇ + Œ≥√óG‚ÇÇ
G‚ÇÄ = r‚ÇÅ + Œ≥√óG‚ÇÅ
```

**Practice:**
```
Rewards: [1, 2, 3], Œ≥ = 0.9

G‚ÇÉ = 0
G‚ÇÇ = 3 + 0.9(0) = 3
G‚ÇÅ = 2 + 0.9(3) = 2 + 2.7 = 4.7
G‚ÇÄ = 1 + 0.9(4.7) = 1 + 4.23 = 5.23
```

## üéØ BELLMAN EQUATIONS (MEMORIZE!)

### Bellman Optimality for V*:
```
V*(s) = max_a [ R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V*(s') ]
```

### Bellman Optimality for Q*:
```
Q*(s,a) = R(s,a) + Œ≥ Œ£_s' P(s'|s,a) max_a' Q*(s',a')
```

## ‚úÖ Hour 2 Checkpoint
- [ ] Can calculate returns from rewards
- [ ] Can write Bellman equation
- [ ] Know V*(s) = max Q*(s,a)

---

# HOUR 3: Dynamic Programming

## üéØ Value Iteration (Most Important!)

```
Repeat:
  For each state s:
    V(s) ‚Üê max_a [ R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V(s') ]
```

**Example - One Step:**
```
2√ó2 Grid, goal at (2,2), R = -1 per step, Œ≥ = 1.0

Initial V = [0, 0, 0, 0]

V(1,1) = max of:
  right: -1 + V(1,2) = -1
  down:  -1 + V(2,1) = -1
  = -1

V(1,2) = max of:
  down ‚Üí goal: 0 + V(goal) = 0  ‚Üê Best!
  = 0
```

## üéØ Policy Iteration

```
1. Evaluate policy: compute V^œÄ
2. Improve policy: œÄ(s) = argmax_a Q(s,a)
3. Repeat until stable
```

## ‚úÖ Hour 3 Checkpoint
- [ ] Can do one value iteration step
- [ ] Understand evaluate ‚Üí improve cycle
- [ ] Know difference: value iter vs policy iter

---

# HOUR 4: Monte Carlo Methods

## üéØ Key Concept: Learn from Episodes

```
Episode: s‚ÇÄ ‚Üí s‚ÇÅ ‚Üí s‚ÇÇ ‚Üí ... ‚Üí Terminal
Returns: G_t = sum of discounted future rewards
```

## üéØ First-Visit vs Every-Visit

```
First-Visit MC: Use return from FIRST visit to state
Every-Visit MC: Use returns from ALL visits
```

**Example:**
```
Episode: A ‚Üí B ‚Üí A ‚Üí C
Returns: G(A first)=10, G(A second)=7

First-Visit:  V(A) = 10
Every-Visit:  V(A) = (10+7)/2 = 8.5
```

## üéØ Œµ-Soft Policy Update

```
After computing Q(s,a):

a* = argmax Q(s,a)

œÄ(a*|s) = 1 - Œµ + Œµ/|A|
œÄ(other|s) = Œµ/|A|
```

## ‚úÖ Hour 4 Checkpoint
- [ ] Know first-visit vs every-visit difference
- [ ] Can update Œµ-soft policy
- [ ] Understand: MC needs complete episodes

---

# HOUR 5: Practice & Review

## üìù Must-Do Problems

### Problem 1: Incremental Update
```
Q = 4.0, Œ± = 0.2, R = 6.0
Q_new = ?

Answer: 4.0 + 0.2(6.0 - 4.0) = 4.0 + 0.4 = 4.4
```

### Problem 2: Return Calculation
```
Rewards: [2, 3, 5], Œ≥ = 0.9
G‚ÇÄ = ?

G‚ÇÇ = 5
G‚ÇÅ = 3 + 0.9(5) = 7.5
G‚ÇÄ = 2 + 0.9(7.5) = 8.75
```

### Problem 3: Œµ-Greedy
```
Œµ = 0.3, |A| = 5, best action = a‚ÇÇ
P(a‚ÇÇ) = ?

P(a‚ÇÇ) = 1 - 0.3 + 0.3/5 = 0.7 + 0.06 = 0.76
```

### Problem 4: Bellman Equation
```
V(s‚ÇÇ) = 10, Œ≥ = 0.9
s‚ÇÅ ‚Üí s‚ÇÇ with R = 5
V(s‚ÇÅ) = ?

V(s‚ÇÅ) = 5 + 0.9(10) = 14
```

### Problem 5: Value Iteration
```
From state A:
  action a‚ÇÅ ‚Üí B with R=2, V(B)=5, Œ≥=0.9
  action a‚ÇÇ ‚Üí C with R=1, V(C)=8, Œ≥=0.9

V(A) = max(2 + 0.9√ó5, 1 + 0.9√ó8)
     = max(6.5, 8.2)
     = 8.2
```

---

## üìã FINAL CHECKLIST

### Formulas to Memorize:
```
1. Q_{n+1} = Q_n + Œ±(R - Q_n)

2. V*(s) = max_a [R(s,a) + Œ≥Œ£P(s'|s,a)V*(s')]

3. G_t = R_{t+1} + Œ≥G_{t+1}

4. P(best) = 1 - Œµ + Œµ/|A|
```

### Concepts to Know:
- [ ] Exploration vs Exploitation
- [ ] Markov Property
- [ ] Why Œ≥ < 1 for infinite horizons
- [ ] On-policy vs Off-policy (MC only)
- [ ] Model-based (DP) vs Model-free (MC)

---

## üéØ EXAM STRATEGY

1. **Read all questions first** - 4 questions
2. **Do numerical problems first** - guaranteed marks
3. **Show all steps** - partial credit
4. **Write formulas** even if stuck on answer
5. **Time: 30min per question**

---

## üìä Quick Reference Values

```
Common Œ≥ values: 0.9, 0.95, 0.99, 1.0
Common Œµ values: 0.1, 0.2, 0.3
Common Œ± values: 0.1, 0.2, 1/n

ln(10) ‚âà 2.3
ln(100) ‚âà 4.6
‚àö2 ‚âà 1.41
‚àö3 ‚âà 1.73
```

---

**You've got this! üí™**
