# üìã DRL Formula Sheet - Quick Reference

> **AIMLCZG512 | Midterm | Sessions 1-8 | Closed Book**

---

## üé∞ MULTI-ARMED BANDITS

### Action Value Estimate (Sample Average)
```
Q_t(a) = (Sum of rewards when a taken) / (Number of times a taken)
```

### Incremental Update ‚≠ê MEMORIZE
```
Q_{n+1} = Q_n + Œ± [R_n - Q_n]

Where:
  Œ± = 1/n     ‚Üí Stationary problems
  Œ± = constant ‚Üí Non-stationary problems
```

### Œµ-Greedy Action Selection
```
With prob (1-Œµ): a = argmax_a Q(a)  [exploit]
With prob Œµ:     a = random action  [explore]
```

### Œµ-Greedy Probabilities
```
P(greedy action) = 1 - Œµ + Œµ/|A|
P(other action)  = Œµ/|A|
```

### UCB Action Selection
```
A_t = argmax_a [ Q_t(a) + c ‚àö(ln t / N_t(a)) ]
```

---

## üìä MDP FUNDAMENTALS

### MDP Tuple
```
(S, A, P, R, Œ≥)

S = State space
A = Action space
P(s'|s,a) = Transition probability
R(s,a) or R(s,a,s') = Reward
Œ≥ ‚àà [0,1] = Discount factor
```

### Return (Discounted Sum)
```
G_t = R_{t+1} + Œ≥R_{t+2} + Œ≥¬≤R_{t+3} + ...

Recursive: G_t = R_{t+1} + Œ≥G_{t+1}
```

### State Value Function
```
V^œÄ(s) = E_œÄ [ Œ£ Œ≥·µè R_{t+k+1} | S_t = s ]
```

### Action Value Function
```
Q^œÄ(s,a) = E_œÄ [ Œ£ Œ≥·µè R_{t+k+1} | S_t = s, A_t = a ]
```

---

## ‚≠ê BELLMAN EQUATIONS (MUST MEMORIZE!)

### Bellman Expectation
```
V^œÄ(s) = Œ£_a œÄ(a|s) [ R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V^œÄ(s') ]
```

### Bellman Optimality
```
V*(s) = max_a [ R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V*(s') ]

Q*(s,a) = R(s,a) + Œ≥ Œ£_s' P(s'|s,a) max_a' Q*(s',a')
```

---

## üîÑ DYNAMIC PROGRAMMING

### Policy Evaluation (Prediction)
```
V(s) ‚Üê Œ£_a œÄ(a|s) [ R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V(s') ]
```

### Value Iteration
```
V(s) ‚Üê max_a [ R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V(s') ]
```

### Policy Improvement
```
œÄ(s) ‚Üê argmax_a [ R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V(s') ]
```

---

## üé≤ MONTE CARLO

### First-Visit MC
```
V(s) = average of returns from FIRST visit to s in each episode
```

### Every-Visit MC
```
V(s) = average of returns from ALL visits to s
```

### MC Update (Incremental)
```
V(s) ‚Üê V(s) + Œ± [ G - V(s) ]

Where G = return from that visit
```

### Œµ-Soft Policy Update
```
a* = argmax_a Q(s,a)

œÄ(a*|s) = 1 - Œµ + Œµ/|A|
œÄ(a‚â†a*|s) = Œµ/|A|
```

---

## üìê QUICK CALCULATIONS

### Return from Rewards
```
Given rewards [r‚ÇÅ, r‚ÇÇ, r‚ÇÉ, ...], Œ≥:

Work backwards:
G_T = 0
G_{t} = r_{t+1} + Œ≥ √ó G_{t+1}
```

### Example:
```
Rewards: [1, 2, 3], Œ≥ = 0.9

G‚ÇÉ = 0
G‚ÇÇ = 3 + 0.9(0) = 3
G‚ÇÅ = 2 + 0.9(3) = 4.7
G‚ÇÄ = 1 + 0.9(4.7) = 5.23
```

### Œµ-Greedy Example
```
Œµ = 0.2, |A| = 4

P(best action) = 1 - 0.2 + 0.2/4 = 0.85
P(other action) = 0.2/4 = 0.05
```

### Incremental Update Example
```
Q = 3.0, Œ± = 0.1, R = 5.0

Q_new = 3.0 + 0.1(5.0 - 3.0)
      = 3.0 + 0.2
      = 3.2
```

---

## üîë KEY RELATIONSHIPS

```
V^œÄ(s) = Œ£_a œÄ(a|s) Q^œÄ(s,a)

Q^œÄ(s,a) = R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V^œÄ(s')

V*(s) = max_a Q*(s,a)

Q*(s,a) = R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V*(s')
```

---

## üìù ALGORITHM COMPARISON

| Method | Model? | Bootstrap? | Episodes? |
|--------|--------|------------|-----------|
| DP | Yes | Yes | No |
| MC | No | No | Yes |
| TD | No | Yes | No |

---

## ‚ö†Ô∏è COMMON MISTAKES

1. **Forgetting Œ≥** in return calculations
2. **Wrong Œ±** for stationary vs non-stationary
3. **Œµ-greedy probability** - remember it's 1-Œµ+Œµ/|A|, not just 1-Œµ
4. **Returns calculated forward** - should be backward from end
5. **Bellman equation missing summation** over states

---

## üéØ EXAM CHECKLIST

- [ ] Can write Bellman optimality equation
- [ ] Can calculate return from reward sequence
- [ ] Can do one step of value iteration
- [ ] Can compute Œµ-greedy probabilities
- [ ] Can do incremental Q update
- [ ] Know first-visit vs every-visit MC difference

---

**Good luck! üçÄ**
