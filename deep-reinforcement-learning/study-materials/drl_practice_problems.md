# üìù DRL Practice Problems - Midterm Prep

> **AIMLCZG512 | 4 Questions √ó 7.5% = 30% | Closed Book**

---

## SECTION 1: Multi-Armed Bandits (Session 2-3)

### Problem 1: Incremental Update ‚≠ê

A 3-armed bandit has the following Q-values after 10 steps:
- Q(a‚ÇÅ) = 2.5 (selected 4 times)
- Q(a‚ÇÇ) = 3.0 (selected 5 times)  
- Q(a‚ÇÉ) = 1.8 (selected 1 time)

You select action a‚ÇÇ and receive reward R = 4.5.

**Questions:**
a) Calculate the new Q(a‚ÇÇ) using sample average method. (2M)
b) Calculate the new Q(a‚ÇÇ) using constant step-size Œ± = 0.1. (2M)
c) Which method would you prefer for a non-stationary problem? Why? (1.5M)

---

**Solution:**

```
a) Sample average method:
   n = 5 (times a‚ÇÇ was selected)
   New n = 6
   Œ± = 1/n = 1/6 ‚âà 0.167
   
   Q_new(a‚ÇÇ) = Q(a‚ÇÇ) + Œ±[R - Q(a‚ÇÇ)]
             = 3.0 + 0.167 √ó [4.5 - 3.0]
             = 3.0 + 0.167 √ó 1.5
             = 3.0 + 0.25
             = 3.25

b) Constant step-size Œ± = 0.1:
   Q_new(a‚ÇÇ) = Q(a‚ÇÇ) + Œ±[R - Q(a‚ÇÇ)]
             = 3.0 + 0.1 √ó [4.5 - 3.0]
             = 3.0 + 0.1 √ó 1.5
             = 3.0 + 0.15
             = 3.15

c) Constant step-size (Œ± = 0.1) is preferred because:
   - It gives more weight to recent rewards
   - Decaying Œ± (1/n) treats all rewards equally
   - Non-stationary means reward distributions change
   - Need to "forget" old experience and adapt
```

---

### Problem 2: Œµ-Greedy Action Selection ‚≠ê

Given Q-values for a 4-armed bandit:
- Q(a‚ÇÅ) = 2.0, Q(a‚ÇÇ) = 5.0, Q(a‚ÇÉ) = 3.5, Q(a‚ÇÑ) = 4.0

With Œµ = 0.2:

**Questions:**
a) What is the probability of selecting each action? (3M)
b) If you use UCB with c = 2, t = 100, and N(a) = [20, 30, 25, 25], which action is selected? (4.5M)

---

**Solution:**

```
a) Œµ-Greedy probabilities:
   Greedy action = a‚ÇÇ (has max Q = 5.0)
   |A| = 4 actions
   
   P(a‚ÇÇ) = 1 - Œµ + Œµ/|A| = 1 - 0.2 + 0.2/4 = 0.8 + 0.05 = 0.85
   P(a‚ÇÅ) = Œµ/|A| = 0.2/4 = 0.05
   P(a‚ÇÉ) = Œµ/|A| = 0.2/4 = 0.05
   P(a‚ÇÑ) = Œµ/|A| = 0.2/4 = 0.05
   
   Verify: 0.85 + 0.05 + 0.05 + 0.05 = 1.0 ‚úì

b) UCB calculation:
   UCB(a) = Q(a) + c √ó ‚àö(ln t / N(a))
   
   ln(100) = 4.605
   
   UCB(a‚ÇÅ) = 2.0 + 2 √ó ‚àö(4.605/20) = 2.0 + 2 √ó ‚àö0.230 = 2.0 + 2 √ó 0.480 = 2.96
   UCB(a‚ÇÇ) = 5.0 + 2 √ó ‚àö(4.605/30) = 5.0 + 2 √ó ‚àö0.154 = 5.0 + 2 √ó 0.392 = 5.78
   UCB(a‚ÇÉ) = 3.5 + 2 √ó ‚àö(4.605/25) = 3.5 + 2 √ó ‚àö0.184 = 3.5 + 2 √ó 0.429 = 4.36
   UCB(a‚ÇÑ) = 4.0 + 2 √ó ‚àö(4.605/25) = 4.0 + 2 √ó ‚àö0.184 = 4.0 + 2 √ó 0.429 = 4.86
   
   Selected action = argmax UCB = a‚ÇÇ (5.78)
```

---

### Problem 3: Non-Stationary Update

For a bandit with Œ± = 0.2, the sequence of rewards for action a‚ÇÅ is: [2, 4, 3, 5].
Initial Q‚ÇÅ(a‚ÇÅ) = 0.

**Calculate Q after each reward.** (6M)

---

**Solution:**

```
Q‚ÇÅ = 0 (initial)

After R‚ÇÅ = 2:
  Q‚ÇÇ = Q‚ÇÅ + Œ±(R‚ÇÅ - Q‚ÇÅ) = 0 + 0.2(2 - 0) = 0.4

After R‚ÇÇ = 4:
  Q‚ÇÉ = Q‚ÇÇ + Œ±(R‚ÇÇ - Q‚ÇÇ) = 0.4 + 0.2(4 - 0.4) = 0.4 + 0.72 = 1.12

After R‚ÇÉ = 3:
  Q‚ÇÑ = Q‚ÇÉ + Œ±(R‚ÇÉ - Q‚ÇÉ) = 1.12 + 0.2(3 - 1.12) = 1.12 + 0.376 = 1.496

After R‚ÇÑ = 5:
  Q‚ÇÖ = Q‚ÇÑ + Œ±(R‚ÇÑ - Q‚ÇÑ) = 1.496 + 0.2(5 - 1.496) = 1.496 + 0.701 = 2.197

Final Q(a‚ÇÅ) = 2.197
```

---

## SECTION 2: Markov Decision Processes (Session 3-5)

### Problem 4: Bellman Equation Calculation ‚≠ê

Consider a simple MDP:
- States: {s‚ÇÅ, s‚ÇÇ, s‚ÇÉ}, where s‚ÇÉ is terminal
- Actions: {a}
- Transitions: s‚ÇÅ --a--> s‚ÇÇ (prob=1, reward=2), s‚ÇÇ --a--> s‚ÇÉ (prob=1, reward=5)
- Œ≥ = 0.9

**Questions:**
a) Write the Bellman equation for V*(s). (2M)
b) Calculate V*(s‚ÇÅ), V*(s‚ÇÇ), V*(s‚ÇÉ). (4M)
c) What is Q*(s‚ÇÅ, a)? (1.5M)

---

**Solution:**

```
a) Bellman Optimality Equation:
   V*(s) = max_a [R(s,a) + Œ≥ Œ£_s' P(s'|s,a) V*(s')]

b) Calculate values (work backwards from terminal):
   
   V*(s‚ÇÉ) = 0 (terminal state)
   
   V*(s‚ÇÇ) = max_a [R(s‚ÇÇ,a) + Œ≥ √ó P(s‚ÇÉ|s‚ÇÇ,a) √ó V*(s‚ÇÉ)]
          = 5 + 0.9 √ó 1 √ó 0
          = 5
   
   V*(s‚ÇÅ) = max_a [R(s‚ÇÅ,a) + Œ≥ √ó P(s‚ÇÇ|s‚ÇÅ,a) √ó V*(s‚ÇÇ)]
          = 2 + 0.9 √ó 1 √ó 5
          = 2 + 4.5
          = 6.5

c) Q*(s‚ÇÅ, a) = R(s‚ÇÅ,a) + Œ≥ √ó P(s‚ÇÇ|s‚ÇÅ,a) √ó V*(s‚ÇÇ)
             = 2 + 0.9 √ó 1 √ó 5
             = 6.5
   
   Note: Q*(s,a) = V*(s) when there's only one action
```

---

### Problem 5: Stochastic MDP ‚≠ê

MDP with states {A, B, C} where C is terminal.
From state A with action a:
- P(B|A,a) = 0.7, R = 3
- P(C|A,a) = 0.3, R = 10

Œ≥ = 0.95, V(B) = 8, V(C) = 0

**Calculate V(A).** (5M)

---

**Solution:**

```
Using Bellman equation:
V(A) = max_a [Œ£_s' P(s'|A,a) √ó (R(A,a,s') + Œ≥ V(s'))]

With only one action a:
V(A) = P(B|A,a) √ó (R_AB + Œ≥ V(B)) + P(C|A,a) √ó (R_AC + Œ≥ V(C))
     = 0.7 √ó (3 + 0.95 √ó 8) + 0.3 √ó (10 + 0.95 √ó 0)
     = 0.7 √ó (3 + 7.6) + 0.3 √ó (10 + 0)
     = 0.7 √ó 10.6 + 0.3 √ó 10
     = 7.42 + 3.0
     = 10.42
```

---

### Problem 6: Return Calculation ‚≠ê

An episode generates the following sequence:
- s‚ÇÄ ‚Üí s‚ÇÅ (r‚ÇÅ = 1) ‚Üí s‚ÇÇ (r‚ÇÇ = 3) ‚Üí s‚ÇÉ (r‚ÇÉ = 2) ‚Üí Terminal (r‚ÇÑ = 10)

With Œ≥ = 0.9:

**Questions:**
a) Calculate G‚ÇÄ (return from s‚ÇÄ). (3M)
b) Calculate G‚ÇÇ (return from s‚ÇÇ). (2M)
c) If this is the only episode and we use first-visit MC, what is V(s‚ÇÅ)? (2.5M)

---

**Solution:**

```
a) Calculate returns from end:
   G‚ÇÑ = 0 (terminal)
   G‚ÇÉ = r‚ÇÑ + Œ≥G‚ÇÑ = 10 + 0.9√ó0 = 10
   G‚ÇÇ = r‚ÇÉ + Œ≥G‚ÇÉ = 2 + 0.9√ó10 = 2 + 9 = 11
   G‚ÇÅ = r‚ÇÇ + Œ≥G‚ÇÇ = 3 + 0.9√ó11 = 3 + 9.9 = 12.9
   G‚ÇÄ = r‚ÇÅ + Œ≥G‚ÇÅ = 1 + 0.9√ó12.9 = 1 + 11.61 = 12.61

b) G‚ÇÇ = 11 (calculated above)

c) First-visit MC:
   V(s‚ÇÅ) = average of returns from first visits to s‚ÇÅ
   
   s‚ÇÅ is visited once at step 1
   Return from s‚ÇÅ = G‚ÇÅ = 12.9
   
   With only one episode:
   V(s‚ÇÅ) = 12.9
```

---

## SECTION 3: Dynamic Programming (Session 4-5)

### Problem 7: Value Iteration ‚≠ê

2√ó2 Gridworld:
```
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ A ‚îÇ B ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ C ‚îÇ G ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
```
- G is goal (terminal, reward = 0)
- All other transitions: reward = -1
- Actions: up, down, left, right
- If action leads outside grid, stay in place
- Œ≥ = 1.0

**Perform one iteration of Value Iteration starting from V(s) = 0 for all s.** (7.5M)

---

**Solution:**

```
Initial: V(A) = V(B) = V(C) = V(G) = 0

For state A:
  up:    ‚Üí A, R = -1, V' = 0 ‚Üí -1 + 1.0√ó0 = -1
  down:  ‚Üí C, R = -1, V' = 0 ‚Üí -1 + 1.0√ó0 = -1
  left:  ‚Üí A, R = -1, V' = 0 ‚Üí -1 + 1.0√ó0 = -1
  right: ‚Üí B, R = -1, V' = 0 ‚Üí -1 + 1.0√ó0 = -1
  V(A) = max(-1, -1, -1, -1) = -1

For state B:
  up:    ‚Üí B, R = -1, V' = 0 ‚Üí -1
  down:  ‚Üí G, R = 0, V' = 0 ‚Üí 0 + 0 = 0  ‚Üê Can reach goal!
  left:  ‚Üí A, R = -1, V' = 0 ‚Üí -1
  right: ‚Üí B, R = -1, V' = 0 ‚Üí -1
  V(B) = max(-1, 0, -1, -1) = 0

For state C:
  up:    ‚Üí A, R = -1, V' = 0 ‚Üí -1
  down:  ‚Üí C, R = -1, V' = 0 ‚Üí -1
  left:  ‚Üí C, R = -1, V' = 0 ‚Üí -1
  right: ‚Üí G, R = 0, V' = 0 ‚Üí 0  ‚Üê Can reach goal!
  V(C) = max(-1, -1, -1, 0) = 0

V(G) = 0 (terminal)

After 1 iteration:
V(A) = -1, V(B) = 0, V(C) = 0, V(G) = 0
```

---

### Problem 8: Policy Evaluation

Given policy œÄ that always moves right, for the same gridworld above.
Calculate V^œÄ(A) after one iteration, starting from V(s) = 0. (5M)

---

**Solution:**

```
Policy œÄ: always move right

State transitions under œÄ:
  A ‚Üí B (R = -1)
  B ‚Üí B (hits wall, stays, R = -1)
  C ‚Üí G (R = 0)
  G is terminal

Policy Evaluation update:
V^œÄ(s) = R(s, œÄ(s)) + Œ≥ √ó V^œÄ(s')

After iteration 1 (starting from all 0):
  V^œÄ(A) = -1 + 1.0 √ó V(B) = -1 + 0 = -1
  V^œÄ(B) = -1 + 1.0 √ó V(B) = -1 + 0 = -1
  V^œÄ(C) = 0 + 1.0 √ó V(G) = 0 + 0 = 0
  
Note: B doesn't reach goal under this policy, so V(B) will stay negative.
```

---

## SECTION 4: Monte Carlo Methods (Session 6-8)

### Problem 9: First-Visit vs Every-Visit MC ‚≠ê

Episode: A ‚Üí B ‚Üí A ‚Üí B ‚Üí C (terminal)
Rewards: r‚ÇÅ=1, r‚ÇÇ=2, r‚ÇÉ=3, r‚ÇÑ=4
Œ≥ = 1.0

**Questions:**
a) Calculate returns for each visit to each state. (3M)
b) What values do first-visit MC assign? (2M)
c) What values do every-visit MC assign? (2.5M)

---

**Solution:**

```
Sequence: A(t=0) ‚Üí B(t=1) ‚Üí A(t=2) ‚Üí B(t=3) ‚Üí C(terminal)
Rewards after each transition: r‚ÇÅ=1, r‚ÇÇ=2, r‚ÇÉ=3, r‚ÇÑ=4

a) Returns (Œ≥ = 1.0):
   G from C: 0
   G from B(t=3): 4 + 0 = 4
   G from A(t=2): 3 + 4 = 7
   G from B(t=1): 2 + 7 = 9
   G from A(t=0): 1 + 9 = 10

b) First-visit MC (only first occurrence):
   V(A) = G from first visit = 10
   V(B) = G from first visit = 9
   V(C) = 0

c) Every-visit MC (average all visits):
   V(A) = average(10, 7) = 8.5
   V(B) = average(9, 4) = 6.5
   V(C) = 0
```

---

### Problem 10: MC Control with Œµ-soft Policy

After one episode, you have:
- Q(s‚ÇÅ, a‚ÇÅ) = 3.0
- Q(s‚ÇÅ, a‚ÇÇ) = 5.0
- Q(s‚ÇÅ, a‚ÇÉ) = 2.0

Using Œµ = 0.3, update the Œµ-soft policy for state s‚ÇÅ. (5M)

---

**Solution:**

```
|A| = 3 actions
Œµ = 0.3

Greedy action = a‚ÇÇ (max Q = 5.0)

Updated policy œÄ(a|s‚ÇÅ):
  œÄ(a‚ÇÇ|s‚ÇÅ) = 1 - Œµ + Œµ/|A| = 1 - 0.3 + 0.3/3 = 0.7 + 0.1 = 0.8
  œÄ(a‚ÇÅ|s‚ÇÅ) = Œµ/|A| = 0.3/3 = 0.1
  œÄ(a‚ÇÉ|s‚ÇÅ) = Œµ/|A| = 0.3/3 = 0.1

Verify: 0.8 + 0.1 + 0.1 = 1.0 ‚úì
```

---

### Problem 11: Incremental MC Update

Using incremental MC update with Œ± = 0.1:
- Current Q(s,a) = 4.5
- New return G = 7.0

**Calculate new Q(s,a).** (3M)

---

**Solution:**

```
Incremental update formula:
Q(s,a) ‚Üê Q(s,a) + Œ±[G - Q(s,a)]

Q_new(s,a) = 4.5 + 0.1 √ó (7.0 - 4.5)
           = 4.5 + 0.1 √ó 2.5
           = 4.5 + 0.25
           = 4.75
```

---

## SECTION 5: Mixed Problems

### Problem 12: Conceptual Questions

a) What is the Markov property? Why is it important? (2M)
b) Difference between on-policy and off-policy learning? (2M)
c) Why do we need exploring starts in MC control? (1.5M)
d) What happens if Œ≥ = 0? Œ≥ = 1? (2M)

---

**Solution:**

```
a) Markov Property:
   The future depends only on current state, not history.
   P(s_{t+1}|s_t, a_t, s_{t-1},...) = P(s_{t+1}|s_t, a_t)
   
   Importance:
   - Allows recursive value function definitions
   - Enables Bellman equations
   - Makes DP and MC methods tractable

b) On-policy vs Off-policy:
   On-policy: Learn about policy currently being used
              (e.g., SARSA, on-policy MC)
   Off-policy: Learn about different policy than one being used
              (e.g., Q-learning, importance sampling MC)

c) Exploring Starts:
   Needed to guarantee all (s,a) pairs are visited.
   Without it, deterministic policy may never explore some actions.
   Ensures convergence to optimal Q-values.

d) Discount factor effects:
   Œ≥ = 0: Only immediate reward matters (myopic)
          G_t = R_{t+1}
   Œ≥ = 1: All future rewards equally important
          May diverge for non-episodic tasks
          Treats $1 today = $1 in 100 years
```

---

### Problem 13: Compare Algorithms

Fill in the table: (6M)

| Aspect | DP | MC | TD |
|--------|----|----|-----|
| Requires model? | | | |
| Bootstraps? | | | |
| Works with episodes only? | | | |

---

**Solution:**

```
| Aspect | DP | MC | TD |
|--------|----|----|-----|
| Requires model? | Yes | No | No |
| Bootstraps? | Yes | No | Yes |
| Works with episodes only? | No | Yes | No |

Explanation:
- DP: Needs P(s'|s,a) and R - full model
- MC: Learns from complete episode returns
- TD: Learns from partial episodes, bootstraps from estimates
```

---

## üìä Answer Key Summary

| Problem | Topic | Key Answer |
|---------|-------|------------|
| 1 | Incremental update | Q_new = 3.25 (sample avg), 3.15 (Œ±=0.1) |
| 2 | Œµ-greedy, UCB | P(a‚ÇÇ) = 0.85; UCB selects a‚ÇÇ |
| 3 | Non-stationary | Final Q = 2.197 |
| 4 | Bellman | V*(s‚ÇÅ) = 6.5 |
| 5 | Stochastic MDP | V(A) = 10.42 |
| 6 | Returns | G‚ÇÄ = 12.61, G‚ÇÇ = 11 |
| 7 | Value iteration | V(A)=-1, V(B)=0, V(C)=0 |
| 8 | Policy evaluation | V^œÄ(A) = -1 |
| 9 | First/Every visit MC | FV: V(A)=10; EV: V(A)=8.5 |
| 10 | Œµ-soft policy | œÄ(a‚ÇÇ)=0.8, œÄ(a‚ÇÅ)=œÄ(a‚ÇÉ)=0.1 |
| 11 | Incremental MC | Q_new = 4.75 |
| 12 | Concepts | Markov, on/off policy, Œ≥ effects |
| 13 | Algorithm comparison | See table |

---

**Good luck! üçÄ**
