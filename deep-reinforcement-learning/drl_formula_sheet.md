# ğŸ“‹ DRL Formula Sheet & Quick Reference

> **Deep Reinforcement Learning | Exam Quick Reference**

---

## ğŸ¯ MDP Components

```
MDP = (S, A, P, R, Î³)

S = State space
A = Action space  
P(s'|s,a) = Transition probability
R(s,a) = Reward function
Î³ = Discount factor (0 â‰¤ Î³ â‰¤ 1)
```

---

## ğŸ“Š Value Functions

### State Value
```
V^Ï€(s) = E[Î£ Î³áµ—râ‚œ | sâ‚€=s, Ï€]
```

### Action Value (Q-value)
```
Q^Ï€(s,a) = E[Î£ Î³áµ—râ‚œ | sâ‚€=s, aâ‚€=a, Ï€]
```

### Relationship
```
V^Ï€(s) = Î£_a Ï€(a|s) Q^Ï€(s,a)
```

---

## â­ BELLMAN EQUATIONS (Memorize!)

### Bellman Expectation (for policy Ï€)
```
V^Ï€(s) = Î£_a Ï€(a|s) [R(s,a) + Î³ Î£_s' P(s'|s,a) V^Ï€(s')]
```

### Bellman Optimality (for optimal V*)
```
V*(s) = max_a [R(s,a) + Î³ Î£_s' P(s'|s,a) V*(s')]

Q*(s,a) = R(s,a) + Î³ Î£_s' P(s'|s,a) max_a' Q*(s',a')
```

---

## ğŸ”„ ALGORITHMS

### Value Iteration
```
V(s) â† max_a [R(s,a) + Î³ Î£ P(s'|s,a) V(s')]
Repeat until convergence
```

### Policy Iteration
```
1. Policy Evaluation: Compute V^Ï€
2. Policy Improvement: Ï€(s) â† argmax_a Q(s,a)
Repeat until policy stable
```

---

## â­ Q-LEARNING (Memorize!)

```
Q(s,a) â† Q(s,a) + Î± [r + Î³ max_a' Q(s',a') - Q(s,a)]
                    \_________target__________/ 
```

### Components
```
Î± = learning rate (e.g., 0.1)
Î³ = discount factor (e.g., 0.9)
r = immediate reward
max_a' Q(s',a') = best Q-value in next state

TD_error = r + Î³ max_a' Q(s',a') - Q(s,a)
```

---

## ğŸ”„ SARSA

```
Q(s,a) â† Q(s,a) + Î± [r + Î³ Q(s',a') - Q(s,a)]
```

### Q-Learning vs SARSA
```
Q-Learning: uses max_a' Q(s',a')  â†’ Off-policy
SARSA:      uses Q(s',a')         â†’ On-policy
```

---

## ğŸ² EXPLORATION

### Îµ-Greedy
```
P(random action) = Îµ
P(greedy action) = 1 - Îµ

Greedy action = argmax_a Q(s,a)
```

### Decay
```
Îµ_t = max(Îµ_min, Îµâ‚€ Ã— decay^t)
```

---

## ğŸ§  DQN (Deep Q-Network)

### Loss Function
```
L = (r + Î³ max_a' Q_target(s',a') - Q(s,a))Â²
```

### Key Techniques
```
1. Experience Replay: Store (s,a,r,s') in buffer
2. Target Network: Separate network for targets
3. Gradient clipping: Prevent exploding gradients
```

---

## ğŸ“ˆ POLICY GRADIENT

### REINFORCE Update
```
Î¸ â† Î¸ + Î± Ã— Gâ‚œ Ã— âˆ‡log Ï€_Î¸(a|s)

Gâ‚œ = râ‚œ + Î³râ‚œâ‚Šâ‚ + Î³Â²râ‚œâ‚Šâ‚‚ + ...
```

### Policy Gradient Theorem
```
âˆ‡J(Î¸) = E[âˆ‡log Ï€_Î¸(a|s) Ã— Q^Ï€(s,a)]
```

---

## ğŸ­ ACTOR-CRITIC

```
Actor:  Updates policy Ï€_Î¸
Critic: Estimates value V_Ï† or Q_Ï†

Advantage: A(s,a) = Q(s,a) - V(s)
```

---

## ğŸ”¢ QUICK CALCULATIONS

### Return Calculation
```
G = râ‚€ + Î³râ‚ + Î³Â²râ‚‚ + ...
Example: r = [1, 2, 3], Î³ = 0.9
G = 1 + 0.9(2) + 0.81(3) = 1 + 1.8 + 2.43 = 5.23
```

### Q-Learning Step
```
Given: Q = 5, Î± = 0.1, Î³ = 0.9, r = 2, max Q' = 8

Target = 2 + 0.9(8) = 9.2
TD_error = 9.2 - 5 = 4.2
Q_new = 5 + 0.1(4.2) = 5.42
```

---

## ğŸ“ COMMON EXAM QUESTIONS

1. **Calculate V(s)** given transitions and rewards
2. **One step of Q-learning** update
3. **Compare Q-learning vs SARSA**
4. **Îµ-greedy action selection**
5. **Bellman equation application**

---

**Focus on: Bellman equations + Q-learning update!**
