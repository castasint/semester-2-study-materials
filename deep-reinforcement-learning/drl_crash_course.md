# ğŸ® Deep Reinforcement Learning - Crash Course

> **5-Hour Crash Course | For Midterm Exam | Core Concepts**

---

## ğŸ“š KEY TOPICS FOR EXAM

1. MDP Fundamentals
2. Bellman Equations
3. Value Iteration & Policy Iteration
4. Q-Learning & SARSA
5. Deep Q-Networks (DQN)
6. Policy Gradient Methods

---

# HOUR 1: MDP Fundamentals

## 1.1 Markov Decision Process (MDP)

An MDP is defined by the tuple: **(S, A, P, R, Î³)**

| Component | Symbol | Description |
|-----------|--------|-------------|
| **States** | S | Set of all possible states |
| **Actions** | A | Set of all possible actions |
| **Transition** | P(s'|s,a) | Probability of reaching s' from s via action a |
| **Reward** | R(s,a,s') | Immediate reward for transition |
| **Discount** | Î³ âˆˆ [0,1] | Future reward discount factor |

## 1.2 Key Definitions

### Policy (Ï€)
```
Ï€(a|s) = Probability of taking action a in state s
```

### State Value Function V^Ï€(s)
```
V^Ï€(s) = Expected total discounted reward starting from s, following Ï€

V^Ï€(s) = E[Râ‚€ + Î³Râ‚ + Î³Â²Râ‚‚ + ... | sâ‚€ = s, Ï€]
       = E[Î£ Î³áµ—Râ‚œ | sâ‚€ = s, Ï€]
```

### Action Value Function Q^Ï€(s,a)
```
Q^Ï€(s,a) = Expected total reward starting from s, taking a, then following Ï€

Q^Ï€(s,a) = E[Î£ Î³áµ—Râ‚œ | sâ‚€ = s, aâ‚€ = a, Ï€]
```

### Relationship
```
V^Ï€(s) = Î£_a Ï€(a|s) Q^Ï€(s,a)

Q^Ï€(s,a) = R(s,a) + Î³ Î£_s' P(s'|s,a) V^Ï€(s')
```

## 1.3 Discount Factor (Î³)

| Î³ Value | Meaning |
|---------|---------|
| Î³ = 0 | Only care about immediate reward |
| Î³ = 1 | Future = Present (may diverge for infinite horizon) |
| Î³ = 0.9 | Standard value, balance short/long term |

---

# HOUR 2: Bellman Equations â­ MOST IMPORTANT

## 2.1 Bellman Expectation Equation

### For V^Ï€(s):
```
V^Ï€(s) = Î£_a Ï€(a|s) [R(s,a) + Î³ Î£_s' P(s'|s,a) V^Ï€(s')]
```

### For Q^Ï€(s,a):
```
Q^Ï€(s,a) = R(s,a) + Î³ Î£_s' P(s'|s,a) Î£_a' Ï€(a'|s') Q^Ï€(s',a')
```

## 2.2 Bellman Optimality Equation

### For V*(s):
```
V*(s) = max_a [R(s,a) + Î³ Î£_s' P(s'|s,a) V*(s')]
```

### For Q*(s,a):
```
Q*(s,a) = R(s,a) + Î³ Î£_s' P(s'|s,a) max_a' Q*(s',a')
```

## 2.3 Worked Example: Gridworld

```
Simple 3-state MDP:
  S = {s1, s2, s3}
  A = {left, right}
  Î³ = 0.9

Transitions (deterministic):
  s1 --right--> s2   (reward = 0)
  s2 --right--> s3   (reward = +10)
  s3 is terminal

Calculate V*(s1):
  V*(s3) = 0 (terminal)
  V*(s2) = max[R(s2,right) + Î³V*(s3)] = 10 + 0.9(0) = 10
  V*(s1) = max[R(s1,right) + Î³V*(s2)] = 0 + 0.9(10) = 9
```

---

# HOUR 3: Value Iteration & Policy Iteration

## 3.1 Value Iteration Algorithm

```
Algorithm: Value Iteration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Initialize V(s) = 0 for all s
2. Repeat until convergence:
   For each state s:
     V(s) â† max_a [R(s,a) + Î³ Î£_s' P(s'|s,a) V(s')]
3. Extract policy:
   Ï€*(s) = argmax_a [R(s,a) + Î³ Î£_s' P(s'|s,a) V(s')]
```

### Convergence
- Î” = max_s |V_new(s) - V_old(s)|
- Stop when Î” < threshold (e.g., 0.001)

## 3.2 Policy Iteration Algorithm

```
Algorithm: Policy Iteration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Initialize Ï€ arbitrarily
2. Repeat until Ï€ stable:
   
   Policy Evaluation:
     Solve V^Ï€(s) = Î£_a Ï€(a|s)[R(s,a) + Î³ Î£_s' P(s'|s,a) V^Ï€(s')]
     (iteratively or by solving linear system)
   
   Policy Improvement:
     For each state s:
       Ï€(s) â† argmax_a [R(s,a) + Î³ Î£_s' P(s'|s,a) V^Ï€(s')]
```

## 3.3 Comparison

| Aspect | Value Iteration | Policy Iteration |
|--------|-----------------|------------------|
| Per iteration | Simple max operation | Full policy evaluation |
| Convergence | Slower iterations | Fewer iterations |
| Memory | Store V only | Store V and Ï€ |

---

# HOUR 4: Q-Learning & SARSA (Model-Free)

## 4.1 Q-Learning Algorithm â­ VERY IMPORTANT

```
Algorithm: Q-Learning (Off-Policy)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Initialize Q(s,a) arbitrarily
For each episode:
  Initialize s
  For each step:
    Choose a from s using policy (e.g., Îµ-greedy)
    Take action a, observe r, s'
    
    Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
    
    s â† s'
  Until s is terminal
```

### Key Update:
```
Q(s,a) â† Q(s,a) + Î± Ã— TD_error

Where: TD_error = r + Î³ max_a' Q(s',a') - Q(s,a)
                = (target) - (current estimate)
```

## 4.2 SARSA Algorithm (On-Policy)

```
Algorithm: SARSA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Initialize Q(s,a) arbitrarily
For each episode:
  Initialize s
  Choose a from s using policy (e.g., Îµ-greedy)
  For each step:
    Take action a, observe r, s'
    Choose a' from s' using policy (Îµ-greedy)
    
    Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
    
    s â† s', a â† a'
  Until s is terminal
```

### Key Difference:
```
Q-Learning: Q(s,a) â† ... + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]  (uses max)
SARSA:      Q(s,a) â† ... + Î±[r + Î³ Q(s',a') - Q(s,a)]        (uses actual a')
```

## 4.3 Îµ-Greedy Policy

```
With probability Îµ: take random action (exploration)
With probability 1-Îµ: take greedy action argmax_a Q(s,a)

Typical: Îµ starts at 1.0, decays to 0.1 over time
```

## 4.4 Worked Example: Q-Learning Update

```
Given:
  Current Q(s,a) = 5.0
  Learning rate Î± = 0.1
  Discount Î³ = 0.9
  Reward r = 2
  max_a' Q(s',a') = 8.0

Calculate new Q(s,a):
  Target = r + Î³ Ã— max_a' Q(s',a')
         = 2 + 0.9 Ã— 8.0
         = 2 + 7.2
         = 9.2
  
  TD_error = Target - Q(s,a) = 9.2 - 5.0 = 4.2
  
  Q_new(s,a) = Q(s,a) + Î± Ã— TD_error
             = 5.0 + 0.1 Ã— 4.2
             = 5.0 + 0.42
             = 5.42
```

---

# HOUR 5: Deep Q-Networks (DQN) & Policy Gradients

## 5.1 Why Deep RL?

| Problem | Solution |
|---------|----------|
| Large state space | Use neural net to approximate Q(s,a) |
| Continuous states | Can't store table, need function |
| Generalization | Learn from similar states |

## 5.2 DQN Architecture

```
Input: State s (e.g., game pixels)
  â†“
Neural Network (CNN/MLP)
  â†“
Output: Q(s,a) for all actions

Loss = (r + Î³ max_a' Q_target(s',a') - Q(s,a))Â²
```

## 5.3 DQN Key Tricks

### Experience Replay
```
- Store transitions (s, a, r, s') in replay buffer
- Sample random mini-batches for training
- Breaks correlation between consecutive samples
```

### Target Network
```
- Separate network Q_target for computing targets
- Copy weights periodically: Q_target â† Q
- Stabilizes training
```

## 5.4 Policy Gradient Basics

Instead of learning Q-values, directly learn the policy Ï€_Î¸(a|s).

### REINFORCE Algorithm
```
For each episode:
  Generate trajectory Ï„ = (sâ‚€, aâ‚€, râ‚€, sâ‚, aâ‚, râ‚, ...)
  
  For each step t:
    G_t = Î£_{k=0}^{T-t} Î³áµ r_{t+k}  (return from step t)
    
    Î¸ â† Î¸ + Î± Ã— G_t Ã— âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t)
```

### Policy Gradient Theorem
```
âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(a|s) Ã— Q^Ï€(s,a)]
```

## 5.5 Actor-Critic

Combines value-based and policy-based:
```
Actor: Policy network Ï€_Î¸(a|s)
Critic: Value network V_Ï†(s) or Q_Ï†(s,a)

Update:
- Critic: Minimize TD error
- Actor: Use critic's value estimate to update policy
```

---

# ğŸ“‹ FORMULA QUICK REFERENCE

## Bellman Equations
```
V*(s) = max_a [R(s,a) + Î³ Î£ P(s'|s,a) V*(s')]
Q*(s,a) = R(s,a) + Î³ Î£ P(s'|s,a) max_a' Q*(s',a')
```

## Q-Learning Update
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
```

## SARSA Update
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
```

## TD Error
```
Î´ = r + Î³V(s') - V(s)    (for state values)
Î´ = r + Î³Q(s',a') - Q(s,a)  (for action values)
```

## Policy Gradient
```
Î¸ â† Î¸ + Î± Ã— G_t Ã— âˆ‡log Ï€_Î¸(a|s)
```

## Return (Discounted Sum)
```
G_t = r_t + Î³r_{t+1} + Î³Â²r_{t+2} + ... = Î£ Î³áµ r_{t+k}
```

---

# ğŸ“ PRACTICE PROBLEMS

## Problem 1: Value Calculation
Given Î³ = 0.9, calculate V(s1) for:
```
s1 --a1--> s2 (r=2)
s2 --a1--> s3 (r=5)
s3 terminal

V(s3) = 0
V(s2) = 5 + 0.9(0) = 5
V(s1) = 2 + 0.9(5) = 2 + 4.5 = 6.5
```

## Problem 2: Q-Learning Update
```
Q(s,a) = 10, Î± = 0.2, Î³ = 0.95, r = 3, max Q(s',a') = 15

Target = 3 + 0.95(15) = 3 + 14.25 = 17.25
TD_error = 17.25 - 10 = 7.25
Q_new = 10 + 0.2(7.25) = 10 + 1.45 = 11.45
```

## Problem 3: Îµ-Greedy
With Îµ = 0.1 and Q-values Q(s,left) = 5, Q(s,right) = 8:
```
P(left) = Îµ/2 = 0.05
P(right) = (1-Îµ) + Îµ/2 = 0.9 + 0.05 = 0.95
```

---

# ğŸ¯ EXAM TIPS

1. **Memorize Bellman equation** - guaranteed to appear
2. **Know Q-learning update step-by-step**
3. **Understand Îµ-greedy exploration**
4. **Practice small gridworld value calculations**
5. **Know difference between on-policy (SARSA) vs off-policy (Q-learning)**

---

**Focus on numerical problems - they give guaranteed marks!**
