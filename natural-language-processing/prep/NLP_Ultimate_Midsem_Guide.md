# ğŸ¯ NLP MIDSEM - SUPER SIMPLE PREP GUIDE
## Explained Like You're 5 Years Old | Score 20+ in 5 Hours

---

# ğŸ—ºï¸ EXAM MAP - WHERE MARKS ARE HIDING

| Q# | What They'll Ask | Type | Marks | Difficulty |
|----|------------------|------|-------|------------|
| 1ï¸âƒ£ | Introduction - NLP Apps | Write sentences | **4** | ğŸ˜Š Easy |
| 2ï¸âƒ£ | Language Models | ğŸ”¢ Calculate | **4** | ğŸ˜ Medium |
| 3ï¸âƒ£ | Neural LM & LLM | Write + Apply | **4** | ğŸ˜Š Easy |
| 4ï¸âƒ£ | Vector Semantics | ğŸ”¢ Calculate | **4** | ğŸ˜ Medium |
| 5ï¸âƒ£ | Word Embeddings | ğŸ”¢ Calculate | **5** | ğŸ˜ Medium |
| 6ï¸âƒ£ | POS Tagging | ğŸ”¢ Calculate | **4** | ğŸ˜ Medium |
| 7ï¸âƒ£ | Viterbi Algorithm | ğŸ”¢ Calculate | **5** | ğŸ˜“ Hard |

**ğŸ¯ Secret**: 26 out of 30 marks = JUST CALCULATIONS. Learn formulas = Win!

---

# â° YOUR 5-HOUR BATTLE PLAN

| Time | What to Study | Expected Marks |
|------|---------------|----------------|
| 11:45 AM - 12:45 PM | Q4 + Q5 (TF-IDF, Cosine, Word2Vec) | +9 |
| 12:45 PM - 1:45 PM | Q6 + Q7 (HMM, Viterbi) | +9 |
| 1:45 PM - 2:45 PM | Q2 + Q3 (N-gram, Perplexity, LLM) | +8 |
| 2:45 PM - 3:45 PM | Q1 (Theory + All Formula Review) | +4 |
| 3:45 PM - 4:45 PM | Practice 5 problems, eat, relax | ğŸ§˜ |

---

# ğŸ“Œ MASTER FORMULA CARD (Screenshot This!)

---

## ğŸ“Š FORMULA 1: TF-IDF

**ğŸ“ Formula:**
```
TF = 1 + logâ‚â‚€(count)        â† count = times word appears in doc
IDF = logâ‚â‚€(N Ã· df)          â† N = total docs, df = docs with word
TF-IDF = TF Ã— IDF
```

**â° When to Use:** Q4 asks "Calculate TF-IDF for word X in document Y"

**ğŸ”¢ Quick Examples:**
```
Example 1: count=5, N=500, df=100
   TF = 1 + log(5) = 1 + 0.7 = 1.7
   IDF = log(500Ã·100) = log(5) = 0.7
   TF-IDF = 1.7 Ã— 0.7 = 1.19 âœ“

Example 2: count=10, N=1000, df=10
   TF = 1 + log(10) = 1 + 1 = 2
   IDF = log(1000Ã·10) = log(100) = 2
   TF-IDF = 2 Ã— 2 = 4.0 âœ“

Example 3: Word appears in ALL docs (df = N)
   IDF = log(NÃ·N) = log(1) = 0
   TF-IDF = anything Ã— 0 = 0 â† Common word = useless!
```

---

## ğŸ“ FORMULA 2: COSINE SIMILARITY

**ğŸ“ Formula:**
```
            Dot Product           aâ‚Ã—bâ‚ + aâ‚‚Ã—bâ‚‚ + ...
Cosine = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Length A Ã— Length B   âˆš(aâ‚Â²+aâ‚‚Â²+...) Ã— âˆš(bâ‚Â²+bâ‚‚Â²+...)
```

**â° When to Use:** Q4 asks "Find similarity between two vectors"

**ğŸ”¢ Quick Examples:**
```
Example 1: A=[3,4], B=[4,3]
   Dot = 3Ã—4 + 4Ã—3 = 24
   Length A = âˆš(9+16) = âˆš25 = 5
   Length B = âˆš(16+9) = âˆš25 = 5
   Cosine = 24 Ã· (5Ã—5) = 24Ã·25 = 0.96 âœ“ (very similar!)

Example 2: A=[1,0], B=[0,1]
   Dot = 1Ã—0 + 0Ã—1 = 0
   Cosine = 0 Ã· anything = 0 âœ“ (perpendicular = nothing in common)

Example 3: A=[2,1,0,2], B=[1,1,2,1]
   Dot = 2Ã—1 + 1Ã—1 + 0Ã—2 + 2Ã—1 = 5
   Length A = âˆš(4+1+0+4) = âˆš9 = 3
   Length B = âˆš(1+1+4+1) = âˆš7 = 2.65
   Cosine = 5 Ã· (3Ã—2.65) = 5Ã·7.95 = 0.63 âœ“
```

---

## ğŸ² FORMULA 3: PERPLEXITY

**ğŸ“ Formula:**
```
PP = (1 Ã· P)^(1/N)    where P = multiply all word probabilities
                            N = number of words
                      
ğŸ’¡ LOWER = BETTER!
```

**â° When to Use:** Q2 asks "Calculate perplexity for this sentence"

**ğŸ”¢ Quick Examples:**
```
Example 1: P(I)=0.4, P(love|I)=0.5, P(NLP|love)=0.2, N=3 words
   P = 0.4 Ã— 0.5 Ã— 0.2 = 0.04
   PP = (1Ã·0.04)^(1/3) = 25^(1/3) = Â³âˆš25 = 2.92 âœ“

Example 2: P=0.0002, N=3
   PP = (1Ã·0.0002)^(1/3) = 5000^(1/3) = Â³âˆš5000 = 17.1 âœ“
   (Higher PP = model more confused)

Example 3: P=0.001, N=4
   PP = (1Ã·0.001)^(1/4) = 1000^(1/4) = â´âˆš1000 = 5.62 âœ“
```

---

## ğŸ“ˆ FORMULA 4: BIGRAM & LAPLACE

**ğŸ“ Formulas:**
```
BIGRAM:   P(word|prev) = Count(prev,word) Ã· Count(prev)

LAPLACE:  P(word|prev) = (Count + 1) Ã· (Count(prev) + VocabSize)
          Use when Count = 0!
```

**â° When to Use:** Q2 asks "Calculate P(word|previous)" or "Apply smoothing"

**ğŸ”¢ Quick Examples:**
```
Example 1: Bigram - C(I,love)=2, C(I)=2
   P(love|I) = 2 Ã· 2 = 1.0 âœ“

Example 2: Bigram - C(I,NLP)=3, C(I)=10
   P(NLP|I) = 3 Ã· 10 = 0.3 âœ“

Example 3: Laplace - C(the,cat)=0, C(the)=50, V=10000
   P(cat|the) = (0+1) Ã· (50+10000) = 1Ã·10050 = 0.0001 âœ“
   (Add 1 to numerator, add vocab to denominator)
```

---

## ğŸ·ï¸ FORMULA 5: HMM SCORE

**ğŸ“ Formula:**
```
Score(tag) = P(tag | prev_tag) Ã— P(word | tag)
             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                TRANSITION         EMISSION

ğŸ¯ Pick the tag with HIGHEST score!
```

**â° When to Use:** Q6 asks "Which tag should this word get?"

**ğŸ”¢ Quick Examples:**
```
Example 1: Word "flies" after a Noun
   
   Try NN (noun):
   Score = P(NN|NN) Ã— P("flies"|NN) = 0.3 Ã— 0.02 = 0.006
   
   Try VBZ (verb):
   Score = P(VBZ|NN) Ã— P("flies"|VBZ) = 0.4 Ã— 0.05 = 0.020
   
   Winner: VBZ âœ“ (0.020 > 0.006)

Example 2: Word "book" after a Verb
   
   Try NN:  Score = 0.5 Ã— 0.04 = 0.020 â† Winner! âœ“
   Try VB:  Score = 0.1 Ã— 0.03 = 0.003
```

---

## ğŸ”„ FORMULA 6: WORD2VEC UPDATE

**ğŸ“ Formula:**
```
Error = Ïƒ(vÂ·u) - y      â† y=1 for real pair, y=0 for fake pair
v_new = v_old - Î· Ã— Error Ã— u

ğŸ’¡ Real pair â†’ vectors move CLOSER
ğŸ’¡ Fake pair â†’ vectors move APART
```

**â° When to Use:** Q5 asks "Update the vector for this word pair"

**ğŸ”¢ Quick Examples:**
```
Example 1: REAL pair (cat, meow), y=1
   Ïƒ(vÂ·u) = 0.55
   Error = 0.55 - 1 = -0.45 (negative = under-predicted)
   v moves TOWARD u âœ“

Example 2: FAKE pair (cat, pizza), y=0
   Ïƒ(vÂ·u) = 0.60
   Error = 0.60 - 0 = +0.60 (positive = over-predicted)
   v moves AWAY from u âœ“

Example 3: Full calculation
   v=[0.2,0.6], u=[0.5,0.3], Ïƒ=0.55, y=1, Î·=0.1
   Error = -0.45
   Gradient = -0.45 Ã— [0.5,0.3] = [-0.225,-0.135]
   v_new = [0.2,0.6] - 0.1Ã—[-0.225,-0.135]
         = [0.2+0.0225, 0.6+0.0135] = [0.2225, 0.6135] âœ“
```

---

## ğŸ” FORMULA 7: WORD ANALOGY

**ğŸ“ Formula:**
```
v_? = v_known - v_old_context + v_new_context

Pattern: A is to B as C is to ?
Formula: ? = C - A + B
```

**â° When to Use:** Q5 asks "Find the vector using analogy"

**ğŸ”¢ Quick Examples:**
```
Example 1: King:Man :: Queen:Woman
   Queen = King - Man + Woman âœ“

Example 2: v_Man=[0.5,0.3], v_Woman=[0.4,0.6], v_King=[0.8,0.4]
   v_Queen = [0.8,0.4] - [0.5,0.3] + [0.4,0.6]
           = [0.8-0.5+0.4, 0.4-0.3+0.6]
           = [0.7, 0.7] âœ“

Example 3: Paris:France :: Tokyo:Japan
   Japan = France - Paris + Tokyo âœ“
```

---

## ğŸ¯ FORMULA 8: VITERBI

**ğŸ“ Formula:**
```
INIT:      Vâ‚(tag) = Ï€(tag) Ã— P(wordâ‚ | tag)
RECURSE:   Vâ‚œ(tag) = max[Vâ‚œâ‚‹â‚(prev) Ã— P(tag|prev)] Ã— P(wordâ‚œ|tag)
BACKTRACK: Start from max final, follow pointers back
```

**â° When to Use:** Q7 asks "Find best tag sequence" or "Complete Viterbi table"

**ğŸ”¢ Quick Example:**
```
Sentence: "The dog runs" | Tags: DT, NN, VBZ

STEP 1 - INIT ("The"):
   Vâ‚(DT) = Ï€(DT) Ã— P("The"|DT) = 0.6 Ã— 0.8 = 0.48 â† Best!
   Vâ‚(NN) = 0.3 Ã— 0.01 = 0.003

STEP 2 - RECURSE ("dog"):
   Vâ‚‚(NN) = Vâ‚(DT) Ã— P(NN|DT) Ã— P("dog"|NN)
          = 0.48 Ã— 0.7 Ã— 0.3 = 0.1008 â† Best!

STEP 3 - RECURSE ("runs"):
   Vâ‚ƒ(VBZ) = Vâ‚‚(NN) Ã— P(VBZ|NN) Ã— P("runs"|VBZ)
           = 0.1008 Ã— 0.7 Ã— 0.4 = 0.028 â† Best!

BACKTRACK: VBZ â† NN â† DT

ANSWER: "The"=DT, "dog"=NN, "runs"=VBZ âœ“
```

---

# ğŸ“— HOUR 1: TF-IDF & COSINE (Q4 = 4 Marks)

---

## ğŸ§’ What is TF-IDF? (Like Explaining to a Kid)

**Imagine you're looking for a book about DOGS in a library:**
- If "dog" appears 100 times in a book â†’ That book is probably about dogs! (High TF)
- If "dog" appears in only 2 out of 1000 books â†’ Word "dog" is SPECIAL! (High IDF)
- If "the" appears in ALL books â†’ Word "the" is BORING, not helpful (Low IDF)

**TF-IDF tells you: How IMPORTANT is this word for THIS document?**

---

## ğŸ”¢ TF-IDF FORMULA (Every Part Explained!)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TF = 1 + logâ‚â‚€(count)                                  â”‚
â”‚  â”€â”€   â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  â”‚    â”‚        â”‚                                        â”‚
â”‚  â”‚    â”‚        â””â”€â”€ How many times word appears in doc   â”‚
â”‚  â”‚    â””â”€â”€ We add 1 so TF is never zero                  â”‚
â”‚  â””â”€â”€ Term Frequency = How often word appears            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IDF = logâ‚â‚€(N Ã· df)                                    â”‚
â”‚  â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚   â”‚         â”‚    â”‚                                      â”‚
â”‚   â”‚         â”‚    â””â”€â”€ df = Document Frequency            â”‚
â”‚   â”‚         â”‚        (how many docs have this word)     â”‚
â”‚   â”‚         â””â”€â”€ N = Total number of documents           â”‚
â”‚   â””â”€â”€ Inverse Doc Freq = Is this word rare/special?     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TF-IDF = TF Ã— IDF                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€   â”€â”€   â”€â”€â”€                                      â”‚
â”‚     â”‚      â”‚    â”‚                                       â”‚
â”‚     â”‚      â”‚    â””â”€â”€ How rare is the word overall?       â”‚
â”‚     â”‚      â””â”€â”€ How often in THIS document?              â”‚
â”‚     â””â”€â”€ Final importance score                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ TF-IDF SOLVED EXAMPLE (Step-by-Step)

**QUESTION**: Calculate TF-IDF for word "machine" in Document D1:
- Word "machine" appears **5 times** in D1
- Total documents = **500**
- "machine" appears in **100** documents

**SOLUTION** (Follow the recipe!):

```
ğŸ¥£ STEP 1: Calculate TF (Term Frequency)
   
   TF = 1 + logâ‚â‚€(count)
   TF = 1 + logâ‚â‚€(5)           â† "machine" appears 5 times
   TF = 1 + 0.699              â† logâ‚â‚€(5) = 0.699 (use calculator)
   TF = 1.699 âœ“
   
   
ğŸ¥£ STEP 2: Calculate IDF (Inverse Document Frequency)

   IDF = logâ‚â‚€(N Ã· df)
   IDF = logâ‚â‚€(500 Ã· 100)      â† 500 total docs, 100 have "machine"
   IDF = logâ‚â‚€(5)              â† 500Ã·100 = 5
   IDF = 0.699 âœ“
   
   
ğŸ¥£ STEP 3: Multiply them!

   TF-IDF = TF Ã— IDF
   TF-IDF = 1.699 Ã— 0.699
   TF-IDF = 1.188 âœ“
   
   
ğŸ“¦ FINAL ANSWER: TF-IDF = 1.188
```

---

## ğŸš¨ SPECIAL CASE: Common Words Like "the"

**QUESTION**: What's TF-IDF for "the" that appears in ALL documents?
- Count in D1 = 20
- Total docs = 500
- "the" appears in = 500 documents (ALL of them!)

**SOLUTION**:
```
TF = 1 + logâ‚â‚€(20) = 1 + 1.301 = 2.301

IDF = logâ‚â‚€(500 Ã· 500) = logâ‚â‚€(1) = 0  â† ZERO!

TF-IDF = 2.301 Ã— 0 = 0 âœ“

ğŸ’¡ INSIGHT: Words in ALL documents are useless for finding 
   specific documents, so TF-IDF = 0!
```

---

## ğŸ§’ What is Cosine Similarity? (Kid-Friendly)

**Imagine two arrows pointing in directions:**
- If both arrows point the SAME way â†’ They're twins! (Similarity = 1)
- If arrows are perpendicular (90Â°) â†’ Nothing in common (Similarity = 0)
- If arrows point OPPOSITE â†’ Total opposites (Similarity = -1)

**We use this to find:** Are two documents similar? Are two words related?

---

## ğŸ”¢ COSINE SIMILARITY FORMULA (Fully Explained)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    A Â· B                                â”‚
â”‚  Cosine = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚            ||A|| Ã— ||B||                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

What does each part mean?

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  A Â· B = Dot Product                                    â”‚
â”‚        = (aâ‚ Ã— bâ‚) + (aâ‚‚ Ã— bâ‚‚) + (aâ‚ƒ Ã— bâ‚ƒ) + ...       â”‚
â”‚                                                         â”‚
â”‚  Think: Multiply matching pairs, then add everything    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ||A|| = Magnitude (Length) of A                        â”‚
â”‚        = âˆš(aâ‚Â² + aâ‚‚Â² + aâ‚ƒÂ² + ...)                      â”‚
â”‚                                                         â”‚
â”‚  Think: Square each number, add them, take square root  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ COSINE SIMILARITY SOLVED EXAMPLE

**QUESTION**: Find similarity between:
- Document A = [2, 1, 0, 2]
- Document B = [1, 1, 2, 1]

**SOLUTION** (Follow the recipe!):

```
ğŸ¥£ STEP 1: Calculate Dot Product (A Â· B)

   Multiply matching pairs, then add:
   
   Position 1: 2 Ã— 1 = 2
   Position 2: 1 Ã— 1 = 1  
   Position 3: 0 Ã— 2 = 0
   Position 4: 2 Ã— 1 = 2
                     â”€â”€â”€â”€
   A Â· B = 2 + 1 + 0 + 2 = 5 âœ“


ğŸ¥£ STEP 2: Calculate ||A|| (Length of A)

   Square each number in A, add them, take âˆš:
   
   A = [2, 1, 0, 2]
   Squares: 4 + 1 + 0 + 4 = 9
   ||A|| = âˆš9 = 3 âœ“


ğŸ¥£ STEP 3: Calculate ||B|| (Length of B)

   B = [1, 1, 2, 1]
   Squares: 1 + 1 + 4 + 1 = 7
   ||B|| = âˆš7 = 2.646 âœ“


ğŸ¥£ STEP 4: Put it all together!

   Cosine = (A Â· B) Ã· (||A|| Ã— ||B||)
   Cosine = 5 Ã· (3 Ã— 2.646)
   Cosine = 5 Ã· 7.938
   Cosine = 0.63 âœ“


ğŸ“¦ FINAL ANSWER: Cosine Similarity = 0.63

ğŸ’¡ MEANING: 0.63 is pretty similar (closer to 1 than to 0)
```

---

# ğŸ“˜ HOUR 1 (Continued): WORD EMBEDDINGS (Q5 = 5 Marks)

---

## ğŸ§’ What is Word2Vec? (Kid-Friendly)

**Imagine every word is a person with a personality:**
- "King" and "Queen" hang out together (royalty friends)
- "Dog" and "Cat" hang out together (pet friends)
- "King" and "Dog" don't hang out much (different groups)

**Word2Vec learns these "friendships" as numbers (vectors)!**

---

## ğŸ”¢ WORD ANALOGY FORMULA (Fully Explained)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  v_Queen = v_King - v_Man + v_Woman                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚     â”‚         â”‚        â”‚        â”‚                       â”‚
â”‚     â”‚         â”‚        â”‚        â””â”€â”€ Add the new context â”‚
â”‚     â”‚         â”‚        â””â”€â”€ Subtract the old context     â”‚
â”‚     â”‚         â””â”€â”€ Start with known word                 â”‚
â”‚     â””â”€â”€ What we want to find                            â”‚
â”‚                                                         â”‚
â”‚  Think: King is to Man as Queen is to Woman            â”‚
â”‚         So: Queen = King - Man + Woman                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ WORD ANALOGY SOLVED EXAMPLE

**QUESTION**: Given these word vectors:
- v_Man = [0.5, 0.3, 0.2]
- v_Woman = [0.4, 0.6, 0.3]
- v_King = [0.8, 0.4, 0.5]

Find v_Queen (King:Man :: Queen:Woman)

**SOLUTION**:
```
ğŸ¥£ STEP 1: Write the formula

   v_Queen = v_King - v_Man + v_Woman


ğŸ¥£ STEP 2: Substitute the numbers

   v_Queen = [0.8, 0.4, 0.5] - [0.5, 0.3, 0.2] + [0.4, 0.6, 0.3]


ğŸ¥£ STEP 3: Do math for each position

   Position 1: 0.8 - 0.5 + 0.4 = 0.7
   Position 2: 0.4 - 0.3 + 0.6 = 0.7
   Position 3: 0.5 - 0.2 + 0.3 = 0.6


ğŸ“¦ FINAL ANSWER: v_Queen = [0.7, 0.7, 0.6]
```

---

## ğŸ§’ Skip-gram vs CBOW (Super Simple!)

### Skip-gram (Target â†’ Context)
```
Given: "cat" (center word)
Predict: "the", "sat", "on", "mat" (surrounding words)

Think: I know the main character, guess who's around them!
```

### CBOW (Context â†’ Target)
```
Given: "the", "sat", "on", "mat" (surrounding words)
Predict: "cat" (center word)

Think: I know the friends, guess the main character!
```

**Memory trick**: 
- **S**kip-gram: **S**ingle word predicts **S**urroundings
- **C**BOW: **C**ontext predicts **C**enter

---

## ğŸ”¢ WORD2VEC UPDATE FORMULA (Fully Explained)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For a word pair, we update the vector like this:       â”‚
â”‚                                                         â”‚
â”‚  Error = Ïƒ(v Â· u) - y                                   â”‚
â”‚  â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€   â”€                                    â”‚
â”‚    â”‚        â”‚      â”‚                                    â”‚
â”‚    â”‚        â”‚      â””â”€â”€ y = 1 if real pair (cat-meow)    â”‚
â”‚    â”‚        â”‚          y = 0 if fake pair (cat-pizza)   â”‚
â”‚    â”‚        â””â”€â”€ Ïƒ(v Â· u) = model's current guess        â”‚
â”‚    â”‚            (probability between 0 and 1)           â”‚
â”‚    â””â”€â”€ How wrong was the model?                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  v_new = v_old - Î· Ã— Error Ã— u                          â”‚
â”‚  â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€   â”€   â”€â”€â”€â”€â”€   â”€                          â”‚
â”‚    â”‚       â”‚     â”‚     â”‚     â”‚                          â”‚
â”‚    â”‚       â”‚     â”‚     â”‚     â””â”€â”€ The other word's vec   â”‚
â”‚    â”‚       â”‚     â”‚     â””â”€â”€ How wrong we were            â”‚
â”‚    â”‚       â”‚     â””â”€â”€ Î· = Learning rate (how big steps)  â”‚
â”‚    â”‚       â”‚         Usually 0.01 to 0.5                â”‚
â”‚    â”‚       â””â”€â”€ Old vector (before update)               â”‚
â”‚    â””â”€â”€ New vector (after update)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ WORD2VEC UPDATE SOLVED EXAMPLE (Positive Pair)

**QUESTION**: Update v_code given:
- Target: "code", Context: "python" (REAL pair, y = 1)
- v_code = [0.2, 0.6]
- u_python = [0.5, 0.3]
- Ïƒ(v Â· u) = 0.55 (model's current guess)
- Î· = 0.1 (learning rate)

**SOLUTION**:
```
ğŸ¥£ STEP 1: Calculate Error (how wrong is the model?)

   Error = Ïƒ(v Â· u) - y
   Error = 0.55 - 1        â† y=1 because it's a real pair
   Error = -0.45 âœ“
   
   ğŸ’¡ Negative error means model under-predicted!


ğŸ¥£ STEP 2: Calculate Gradient (direction to move)

   Gradient = Error Ã— u_python
   Gradient = -0.45 Ã— [0.5, 0.3]
   Gradient = [-0.225, -0.135] âœ“


ğŸ¥£ STEP 3: Update the vector

   v_new = v_old - Î· Ã— Gradient
   v_new = [0.2, 0.6] - 0.1 Ã— [-0.225, -0.135]
   v_new = [0.2, 0.6] - [-0.0225, -0.0135]
   v_new = [0.2 + 0.0225, 0.6 + 0.0135]    â† minus a negative = plus!
   v_new = [0.2225, 0.6135] âœ“


ğŸ“¦ FINAL ANSWER: v_code_new = [0.2225, 0.6135]

ğŸ’¡ INSIGHT: For REAL pairs, vectors move CLOSER together!
```

---

## ğŸ“ WORD2VEC UPDATE SOLVED EXAMPLE (Negative Pair)

**QUESTION**: Update v_dog given:
- Target: "dog", Context: "pizza" (FAKE pair, y = 0)
- v_dog = [0.4, 0.8]
- u_pizza = [0.6, 0.2]
- Ïƒ(v Â· u) = 0.60
- Î· = 0.2

**SOLUTION**:
```
ğŸ¥£ STEP 1: Error = 0.60 - 0 = 0.60 â† y=0 for fake pair

ğŸ¥£ STEP 2: Gradient = 0.60 Ã— [0.6, 0.2] = [0.36, 0.12]

ğŸ¥£ STEP 3: v_new = [0.4, 0.8] - 0.2 Ã— [0.36, 0.12]
                 = [0.4 - 0.072, 0.8 - 0.024]
                 = [0.328, 0.776] âœ“


ğŸ“¦ FINAL ANSWER: v_dog_new = [0.328, 0.776]

ğŸ’¡ INSIGHT: For FAKE pairs, vectors move APART from each other!
```

---

# ğŸ“™ HOUR 2: HMM & VITERBI (Q6 + Q7 = 9 Marks)

---

## ğŸ§’ What is POS Tagging? (Kid-Friendly)

**Every word has a job in a sentence:**
- "dog" â†’ Noun (NN) - a thing
- "runs" â†’ Verb (VBZ) - an action
- "the" â†’ Determiner (DT) - points to something
- "happy" â†’ Adjective (JJ) - describes something

**The computer needs to figure out each word's job!**

---

## ğŸ§’ What is HMM? (Super Simple!)

**Think of it like a guessing game:**

1. You can't see the TAGS (they're hidden) ğŸ™ˆ
2. You CAN see the WORDS (they're visible) ğŸ‘€
3. You use CLUES to guess the tags:
   - **Clue 1**: What tag usually comes after the previous tag?
   - **Clue 2**: What tag usually makes this word?

---

## ğŸ”¢ HMM FORMULA (Fully Explained)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  Score = P(tag | prev_tag) Ã— P(word | tag)             â”‚
â”‚  â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚    â”‚            â”‚                   â”‚                   â”‚
â”‚    â”‚            â”‚                   â””â”€â”€ EMISSION:       â”‚
â”‚    â”‚            â”‚                       If this IS a    â”‚
â”‚    â”‚            â”‚                       noun, how often â”‚
â”‚    â”‚            â”‚                       is it "book"?   â”‚
â”‚    â”‚            â”‚                                       â”‚
â”‚    â”‚            â””â”€â”€ TRANSITION:                         â”‚
â”‚    â”‚                After a verb, how often             â”‚
â”‚    â”‚                does a noun come next?              â”‚
â”‚    â”‚                                                    â”‚
â”‚    â””â”€â”€ Final score - higher is better!                  â”‚
â”‚                                                         â”‚
â”‚  ğŸ¯ CHOOSE THE TAG WITH HIGHEST SCORE!                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ HMM SOLVED EXAMPLE (Word Disambiguation)

**QUESTION**: The word "flies" comes after a NOUN. What tag should "flies" get?
- Candidate tags: NN (noun) or VBZ (verb)
- P(NN | NN) = 0.3 â† Probability noun follows noun
- P(VBZ | NN) = 0.4 â† Probability verb follows noun
- P("flies" | NN) = 0.02 â† If it's a noun, how often is it "flies"
- P("flies" | VBZ) = 0.05 â† If it's a verb, how often is it "flies"

**SOLUTION**:
```
ğŸ¥£ STEP 1: Calculate score for NN (noun)

   Score(NN) = P(NN | NN) Ã— P("flies" | NN)
             = 0.3 Ã— 0.02
             = 0.006 âœ“


ğŸ¥£ STEP 2: Calculate score for VBZ (verb)

   Score(VBZ) = P(VBZ | NN) Ã— P("flies" | VBZ)
              = 0.4 Ã— 0.05
              = 0.020 âœ“


ğŸ¥£ STEP 3: Compare and pick the winner!

   Score(NN)  = 0.006
   Score(VBZ) = 0.020  â† BIGGER = WINNER! ğŸ†


ğŸ“¦ FINAL ANSWER: "flies" = VBZ (verb)

ğŸ’¡ MEANING: "Time flies" â†’ flies is an action verb!
   (Not the insect noun)
```

---

## ğŸ§’ What is Viterbi? (Super Simple!)

**Viterbi is like finding the best path through a maze:**
- At each step, you calculate ALL possible scores
- You remember which path gave the best score
- At the end, you trace back to find the winning path!

**Three simple steps:**
1. **START**: Calculate scores for first word
2. **CONTINUE**: For each next word, find best path to each tag
3. **TRACE BACK**: Follow the winning path backwards

---

## ğŸ”¢ VITERBI FORMULA (Fully Explained)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 - INITIALIZATION (First word only)              â”‚
â”‚                                                         â”‚
â”‚  Vâ‚(tag) = Ï€(tag) Ã— P(wordâ‚ | tag)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚     â”‚        â”‚            â”‚                             â”‚
â”‚     â”‚        â”‚            â””â”€â”€ Emission: how likely      â”‚
â”‚     â”‚        â”‚                this word for this tag?   â”‚
â”‚     â”‚        â””â”€â”€ Ï€ = Start probability                  â”‚
â”‚     â”‚            (how often does sentence start         â”‚
â”‚     â”‚             with this tag?)                       â”‚
â”‚     â””â”€â”€ V = Viterbi score for first position            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 - RECURSION (All other words)                   â”‚
â”‚                                                         â”‚
â”‚  Vâ‚œ(j) = max[Vâ‚œâ‚‹â‚(i) Ã— A(iâ†’j)] Ã— B(j, wordâ‚œ)           â”‚
â”‚  â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚    â”‚              â”‚                    â”‚                â”‚
â”‚    â”‚              â”‚                    â””â”€â”€ Emission     â”‚
â”‚    â”‚              â”‚                        P(word|tag)  â”‚
â”‚    â”‚              â””â”€â”€ Find the BEST previous path       â”‚
â”‚    â”‚                  Try all previous tags, pick max   â”‚
â”‚    â””â”€â”€ Score at time t for tag j                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ VITERBI COMPLETE SOLVED EXAMPLE

**QUESTION**: Tag the sentence "The dog runs"
- Tags available: DT (determiner), NN (noun), VBZ (verb)

**GIVEN DATA**:

**Start Probabilities** (how often sentences start with each tag):
| Tag | Ï€ (start prob) |
|-----|----------------|
| DT | 0.6 |
| NN | 0.3 |
| VBZ | 0.1 |

**Transition Probabilities** (what tag follows what):
| From â†“ To â†’ | DT | NN | VBZ |
|-------------|-----|-----|-----|
| DT | 0.1 | 0.7 | 0.2 |
| NN | 0.1 | 0.2 | 0.7 |
| VBZ | 0.3 | 0.6 | 0.1 |

**Emission Probabilities** (word given tag):
| Word | P(word\|DT) | P(word\|NN) | P(word\|VBZ) |
|------|------------|------------|-------------|
| The | 0.8 | 0.01 | 0.01 |
| dog | 0.01 | 0.3 | 0.02 |
| runs | 0.01 | 0.05 | 0.4 |

---

### ğŸ¥£ STEP 1: INITIALIZATION (Word = "The")

```
Calculate Vâ‚ for each possible tag:

Vâ‚(DT) = Ï€(DT) Ã— P("The"|DT) 
       = 0.6 Ã— 0.8 
       = 0.48 â­ HIGHEST!

Vâ‚(NN) = Ï€(NN) Ã— P("The"|NN) 
       = 0.3 Ã— 0.01 
       = 0.003

Vâ‚(VBZ) = Ï€(VBZ) Ã— P("The"|VBZ) 
        = 0.1 Ã— 0.01 
        = 0.001

ğŸ“Š After "The": DT is winning with 0.48
```

---

### ğŸ¥£ STEP 2A: RECURSION (Word = "dog")

**For NN, try coming from each previous tag:**
```
From DT:  Vâ‚(DT) Ã— P(NN|DT) = 0.48 Ã— 0.7 = 0.336 â† BEST PATH!
From NN:  Vâ‚(NN) Ã— P(NN|NN) = 0.003 Ã— 0.2 = 0.0006
From VBZ: Vâ‚(VBZ) Ã— P(NN|VBZ) = 0.001 Ã— 0.6 = 0.0006

Vâ‚‚(NN) = 0.336 Ã— P("dog"|NN) = 0.336 Ã— 0.3 = 0.1008 â­
Backpointer: DT (came from DT)
```

**For VBZ:**
```
Best path: From DT: 0.48 Ã— 0.2 = 0.096
Vâ‚‚(VBZ) = 0.096 Ã— P("dog"|VBZ) = 0.096 Ã— 0.02 = 0.00192
```

---

### ğŸ¥£ STEP 2B: RECURSION (Word = "runs")

**For VBZ, try coming from each previous tag:**
```
From DT:  Vâ‚‚(DT) Ã— P(VBZ|DT) = 0.00048 Ã— 0.2 = 0.000096
From NN:  Vâ‚‚(NN) Ã— P(VBZ|NN) = 0.1008 Ã— 0.7 = 0.07056 â† BEST!
From VBZ: Vâ‚‚(VBZ) Ã— P(VBZ|VBZ) = 0.00192 Ã— 0.1 = 0.000192

Vâ‚ƒ(VBZ) = 0.07056 Ã— P("runs"|VBZ) = 0.07056 Ã— 0.4 = 0.02822 â­
Backpointer: NN (came from NN)
```

---

### ğŸ¥£ STEP 3: BACKTRACKING

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final scores for "runs":             â”‚
â”‚  VBZ = 0.02822 â† WINNER! ğŸ†           â”‚
â”‚                                       â”‚
â”‚  Trace back:                          â”‚
â”‚  "runs" â†’ VBZ (best=0.02822)          â”‚
â”‚     â†‘ came from                       â”‚
â”‚  "dog" â†’ NN (backpointer said NN)     â”‚
â”‚     â†‘ came from                       â”‚
â”‚  "The" â†’ DT (backpointer said DT)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ“Š FINAL VITERBI TABLE

| Tag | "The" | "dog" | "runs" |
|-----|-------|-------|--------|
| DT | **0.48** | 0.00048 | - |
| NN | 0.003 | **0.1008** â†DT | - |
| VBZ | 0.001 | 0.00192 | **0.02822** â†NN |

**ğŸ“¦ FINAL ANSWER**: **DT â†’ NN â†’ VBZ**
- "The" = Determiner
- "dog" = Noun
- "runs" = Verb

---

# ğŸ“• HOUR 3: LANGUAGE MODELS (Q2 = 4 Marks)

---

## ğŸ§’ What is a Language Model? (Kid-Friendly)

**It predicts what word comes next!**

Example: "I love ___"
- A good model might guess: "you" (80%), "pizza" (10%), "coding" (5%)...
- A bad model might guess: "the" (50%), "banana" (50%)

---

## ğŸ”¢ BIGRAM FORMULA (Fully Explained)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  P(word | prev) = Count(prev, word) Ã· Count(prev)      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚        â”‚                â”‚                  â”‚            â”‚
â”‚        â”‚                â”‚                  â””â”€â”€ How many â”‚
â”‚        â”‚                â”‚                      times didâ”‚
â”‚        â”‚                â”‚                      prev     â”‚
â”‚        â”‚                â”‚                      appear?  â”‚
â”‚        â”‚                â””â”€â”€ How many times did we see   â”‚
â”‚        â”‚                    these two words together?   â”‚
â”‚        â””â”€â”€ Probability of "word" following "prev"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ BIGRAM SOLVED EXAMPLE

**QUESTION**: Given this corpus, find P(love | I):
```
<s> I love NLP </s>
<s> I love coding </s>
<s> NLP is fun </s>
```

**SOLUTION**:
```
ğŸ¥£ STEP 1: Count how many times "I" appears

   Sentence 1: "I" appears once
   Sentence 2: "I" appears once
   Sentence 3: "I" doesn't appear
   
   Count(I) = 2 âœ“


ğŸ¥£ STEP 2: Count how many times "I love" appears together

   Sentence 1: "I love" âœ“
   Sentence 2: "I love" âœ“
   
   Count(I, love) = 2 âœ“


ğŸ¥£ STEP 3: Apply the formula

   P(love | I) = Count(I, love) Ã· Count(I)
               = 2 Ã· 2
               = 1.0 âœ“


ğŸ“¦ FINAL ANSWER: P(love | I) = 1.0 (or 100%)

ğŸ’¡ MEANING: Every time we saw "I", it was followed by "love"!
```

---

## ğŸ”¢ LAPLACE SMOOTHING FORMULA (Fully Explained)

**Problem**: What if we never saw "I cat" together? P = 0Ã·2 = 0!

**Solution**: Add 1 to everything (fake it till you make it!)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  P_smooth = (Count + 1) Ã· (N + V)                      â”‚
â”‚             â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                 â”‚            â”‚â”€â”€â”€ V = Vocabulary size   â”‚
â”‚                 â”‚            â”‚    (total unique words)  â”‚
â”‚                 â”‚            â””â”€â”€ N = How many times     â”‚
â”‚                 â”‚                prev word appeared     â”‚
â”‚                 â””â”€â”€ Add 1 to the count (even if 0!)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ LAPLACE SMOOTHING SOLVED EXAMPLE

**QUESTION**: Calculate P(cat | the) with Laplace smoothing:
- Count("the", "cat") = 0 (never saw them together!)
- Count("the") = 50
- Vocabulary size V = 10,000

**SOLUTION**:
```
ğŸ¥£ Apply Laplace formula:

   P_smooth(cat | the) = (Count + 1) Ã· (N + V)
                       = (0 + 1) Ã· (50 + 10,000)
                       = 1 Ã· 10,050
                       = 0.0000995 âœ“


ğŸ“¦ FINAL ANSWER: P â‰ˆ 0.0001

ğŸ’¡ MEANING: Now it's not zero! Small, but possible!
```

---

## ğŸ”¢ PERPLEXITY FORMULA (Fully Explained)

### ğŸ“Œ FORMULA BOX:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  PP = (1 Ã· P(sentence))^(1Ã·N)                          â”‚
â”‚  â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€                           â”‚
â”‚  â”‚           â”‚            â”‚                             â”‚
â”‚  â”‚           â”‚            â””â”€â”€ N = number of words       â”‚
â”‚  â”‚           â””â”€â”€ Total probability of sentence          â”‚
â”‚  â”‚               (multiply all word probabilities)      â”‚
â”‚  â””â”€â”€ Perplexity = "how confused is the model?"          â”‚
â”‚                                                         â”‚
â”‚  ğŸ’¡ LOWER PERPLEXITY = BETTER MODEL!                    â”‚
â”‚  ğŸ’¡ PP of 10 means choosing from ~10 equally likely     â”‚
â”‚     words at each position                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ PERPLEXITY SOLVED EXAMPLE

**QUESTION**: Calculate perplexity for "I love NLP" (N=3 words):
- P(I | start) = 0.4
- P(love | I) = 0.5
- P(NLP | love) = 0.2

**SOLUTION**:
```
ğŸ¥£ STEP 1: Calculate P(sentence)

   P(sentence) = P(I) Ã— P(love|I) Ã— P(NLP|love)
               = 0.4 Ã— 0.5 Ã— 0.2
               = 0.04 âœ“


ğŸ¥£ STEP 2: Calculate (1 Ã· P)

   1 Ã· 0.04 = 25


ğŸ¥£ STEP 3: Take the Nth root (N=3)

   PP = 25^(1/3)        â† Cube root of 25
   PP = Â³âˆš25
   PP â‰ˆ 2.92 âœ“


ğŸ“¦ FINAL ANSWER: Perplexity â‰ˆ 2.92

ğŸ’¡ MEANING: Model is choosing from about 3 words at each step.
            That's pretty good! (lower = better)
```

---

# ğŸ“’ HOUR 4: THEORY (Q1 + Q3)

---

## Q1: NLP APPLICATIONS (Memorize 4!)

| Application | What it does | Example |
|-------------|--------------|---------|
| ğŸŒ Machine Translation | Language A â†’ Language B | Google Translate |
| ğŸ˜Š Sentiment Analysis | Is text happy/sad/angry? | Product reviews |
| ğŸ·ï¸ NER | Find names, places, orgs | "Apple is in California" |
| â“ Question Answering | Answer questions | Siri, Alexa |

---

## Q1: LEVELS OF LANGUAGE (Memorize Order!)

```
ğŸ­ Pragmatic    = "Can you pass salt?" means PLEASE PASS IT
    â†“
ğŸ“– Discourse    = "John went. He bought" - He = John  
    â†“
ğŸ’­ Semantic     = Word meanings
    â†“
ğŸ“ Syntactic    = Grammar rules
    â†“
ğŸ·ï¸ Lexical      = Word categories (noun, verb)
    â†“
ğŸ”¤ Morphological = Word parts (un + happy + ness)
```

**Memory Trick**: **P**lease **D**on't **S**leep, **S**tudy **L**ate **M**orning

---

## Q1: TYPES OF AMBIGUITY

| Type | Example | Why ambiguous? |
|------|---------|----------------|
| **Structural** | "I saw man with telescope" | WHO has the telescope? |
| **Lexical** | "The bank is flooded" | River bank? Money bank? |
| **Grammatical** | "Can you can a can?" | can = ability/verb/noun |

---

## Q3: PROMPT TYPES

| Type | How many examples? | Example |
|------|-------------------|---------|
| **Zero-shot** | 0 | "Translate: Hello â†’ French" |
| **One-shot** | 1 | "Helloâ†’Bonjour. Goodbyeâ†’?" |
| **Few-shot** | 2-5 | Multiple examples first |
| **Chain-of-Thought** | Step by step | "Let's solve step by step..." |

---

# âš ï¸ COMMON MISTAKES - DON'T DO THESE!

| âŒ MISTAKE | âœ… CORRECT |
|-----------|-----------|
| TF = logâ‚â‚€(count) | TF = **1 +** logâ‚â‚€(count) |
| PP = P^(1/N) | PP = P^(**-1/N**) or (1/P)^(1/N) |
| HMM = just transition | HMM = Transition **Ã—** Emission |
| Viterbi: forgot emission | Must multiply by **P(word\|tag)** at end! |
| Cosine: forgot magnitude | Calculate **BOTH** ||A|| **AND** ||B|| |
| Skip-gram = contextâ†’target | Skip-gram = **targetâ†’context** |

---

# ğŸ† EXAM STRATEGY

**Answer in this order:**
1. **Q4** (TF-IDF, Cosine) - 4 marks - Direct calculation
2. **Q5** (Word2Vec) - 5 marks - Formula-based
3. **Q6** (HMM) - 4 marks - Multiplication only
4. **Q7** (Viterbi) - 5 marks - Takes time, do carefully
5. **Q2** (N-gram, PP) - 4 marks - Easy formulas
6. **Q3** (LLM) - 4 marks - Theory, relax
7. **Q1** (Intro) - 4 marks - Just write points

**ğŸ€ SHOW ALL STEPS = PARTIAL MARKS!**

---

# ğŸ“ GOOD LUCK! YOU'VE GOT THIS!
