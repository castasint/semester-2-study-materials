# Session 7: Statistical, ML, and Neural Models for POS Tagging
## AIMLCZG530 - Natural Language Processing

---

# 1. Three Fundamental HMM Problems

| Problem | Question | Algorithm |
|---------|----------|-----------|
| **Likelihood** | P(observations \| model) | Forward Algorithm |
| **Decoding** | Best state sequence | Viterbi Algorithm |
| **Learning** | Estimate parameters | Baum-Welch (EM) |

For POS tagging, we focus on **Decoding** (Viterbi).

---

# 2. The Forward Algorithm

## 2.1 Purpose
Compute probability of observation sequence: P(O | Œª)

## 2.2 Intuition
Sum over all possible state sequences (too expensive to enumerate)

## 2.3 Forward Variable
```
Œ±‚Çú(j) = P(o‚ÇÅ, o‚ÇÇ, ..., o‚Çú, q‚Çú = j | Œª)
```

Probability of seeing observations o‚ÇÅ...o‚Çú AND being in state j at time t.

## 2.4 Algorithm

**Initialization (t = 1)**:
```
Œ±‚ÇÅ(j) = œÄ(j) √ó B(j, o‚ÇÅ)
```

**Recursion (t > 1)**:
```
Œ±‚Çú(j) = [Œ£·µ¢ Œ±‚Çú‚Çã‚ÇÅ(i) √ó A(i,j)] √ó B(j, o‚Çú)
```

**Termination**:
```
P(O | Œª) = Œ£‚±º Œ±‚Çú(j)
```

---

# 3. The Viterbi Algorithm

## 3.1 Purpose
Find the most likely sequence of hidden states (POS tags).

## 3.2 Key Insight
Use **dynamic programming** instead of enumerating all paths.

**Complexity**: O(N¬≤ √ó T)
- N = number of states (tags)
- T = sequence length

## 3.3 Viterbi Variable

```
V‚Çú(j) = max P(q‚ÇÅ, q‚ÇÇ, ..., q‚Çú = j, o‚ÇÅ, o‚ÇÇ, ..., o‚Çú | Œª)
```

Probability of best path ending in state j at time t.

## 3.4 Algorithm Steps

### Step 1: Initialization (t = 1)
```
V‚ÇÅ(j) = œÄ(j) √ó B(j, o‚ÇÅ)
bp‚ÇÅ(j) = 0  (no backpointer for first state)
```

### Step 2: Recursion (t = 2 to T)
```
V‚Çú(j) = max[V‚Çú‚Çã‚ÇÅ(i) √ó A(i,j)] √ó B(j, o‚Çú)
bp‚Çú(j) = argmax[V‚Çú‚Çã‚ÇÅ(i) √ó A(i,j)]
```

### Step 3: Termination
```
Best final state: q*‚Çú = argmax V‚Çú(j)
```

### Step 4: Backtracking
```
q*‚Çú‚Çã‚ÇÅ = bp‚Çú(q*‚Çú)
q*‚Çú‚Çã‚ÇÇ = bp‚Çú‚Çã‚ÇÅ(q*‚Çú‚Çã‚ÇÅ)
... and so on
```

## 3.5 Complete Example

**Sentence**: "I run"
**States**: PRP, VB, NN

**Parameters**:
```
Start probabilities:
œÄ(PRP) = 0.6, œÄ(VB) = 0.2, œÄ(NN) = 0.2

Transition matrix:
       PRP   VB   NN
PRP    0.1   0.6   0.3
VB     0.2   0.2   0.6
NN     0.3   0.4   0.3

Emission probabilities:
P("I" | PRP) = 0.9, P("I" | VB) = 0, P("I" | NN) = 0
P("run" | PRP) = 0, P("run" | VB) = 0.4, P("run" | NN) = 0.2
```

### Step 1: Initialization (word = "I")

```
V‚ÇÅ(PRP) = œÄ(PRP) √ó P("I"|PRP) = 0.6 √ó 0.9 = 0.54
V‚ÇÅ(VB) = œÄ(VB) √ó P("I"|VB) = 0.2 √ó 0 = 0
V‚ÇÅ(NN) = œÄ(NN) √ó P("I"|NN) = 0.2 √ó 0 = 0
```

### Step 2: Recursion (word = "run")

**For VB**:
```
From PRP: 0.54 √ó 0.6 = 0.324
From VB: 0 √ó 0.2 = 0
From NN: 0 √ó 0.4 = 0
Max = 0.324 (from PRP)
V‚ÇÇ(VB) = 0.324 √ó P("run"|VB) = 0.324 √ó 0.4 = 0.1296
bp‚ÇÇ(VB) = PRP
```

**For NN**:
```
From PRP: 0.54 √ó 0.3 = 0.162
From VB: 0 √ó 0.6 = 0
From NN: 0 √ó 0.3 = 0
Max = 0.162 (from PRP)
V‚ÇÇ(NN) = 0.162 √ó P("run"|NN) = 0.162 √ó 0.2 = 0.0324
bp‚ÇÇ(NN) = PRP
```

**For PRP**:
```
Max = 0.054 √ó 0 = 0
```

### Step 3: Termination
```
Best final state: VB (0.1296 > 0.0324 > 0)
```

### Step 4: Backtracking
```
q‚ÇÇ* = VB
q‚ÇÅ* = bp‚ÇÇ(VB) = PRP
```

**Result**: PRP ‚Üí VB ("I" = pronoun, "run" = verb)

### Viterbi Table

| State | "I" | "run" | Backpointer |
|-------|-----|-------|-------------|
| PRP | 0.54 | 0 | - |
| VB | 0 | 0.1296 | PRP |
| NN | 0 | 0.0324 | PRP |

---

# 4. Log Probabilities in Viterbi

## 4.1 Problem
Products of small probabilities ‚Üí underflow

## 4.2 Solution
Work in log space:
```
log(a √ó b) = log(a) + log(b)
```

## 4.3 Log Viterbi

**Initialization**:
```
log V‚ÇÅ(j) = log œÄ(j) + log B(j, o‚ÇÅ)
```

**Recursion**:
```
log V‚Çú(j) = max[log V‚Çú‚Çã‚ÇÅ(i) + log A(i,j)] + log B(j, o‚Çú)
```

**Example**:
```
log V‚ÇÅ(PRP) = log(0.6) + log(0.9) = -0.22 + (-0.05) = -0.27
log V‚ÇÇ(VB) = log(0.54) + log(0.6) + log(0.4) = -0.27 + (-0.22) + (-0.40) = -0.89
```

---

# 5. Maximum Entropy Markov Model (MEMM)

## 5.1 HMM Limitations

| Limitation | Description |
|------------|-------------|
| **Limited features** | Only word identity |
| **Independence** | Observations independent given states |
| **Generative** | Models P(word \| tag) not P(tag \| word) |

## 5.2 MEMM: Discriminative Approach

**Key difference**:
- HMM: P(word | tag) √ó P(tag | prev_tag) ‚Äî Generative
- MEMM: P(tag | word, prev_tag, features) ‚Äî Discriminative

## 5.3 Features in MEMM

| Feature Type | Examples |
|--------------|----------|
| **Current word** | word = "running" |
| **Previous tag** | prev_tag = VB |
| **Word suffix** | suffix = "-ing" |
| **Word prefix** | prefix = "un-" |
| **Capitalization** | is_capitalized = True |
| **Contains digit** | has_digit = False |
| **Previous word** | prev_word = "is" |
| **Next word** | next_word = "fast" |
| **Word shape** | shape = "Xxxxx" |

## 5.4 Maximum Entropy Model

**Formula**:
```
P(tag | features) = exp(Œ£·µ¢ w·µ¢ √ó f·µ¢) / Z
```

Where:
- w·µ¢ = learned weights
- f·µ¢ = feature functions (0 or 1)
- Z = normalization constant

## 5.5 Example Features

For word "unhappiness" with previous tag = JJ:

| Feature | Value |
|---------|-------|
| word = "unhappiness" | 1 |
| prev_tag = JJ | 1 |
| suffix = "-ness" | 1 |
| prefix = "un-" | 1 |
| length > 8 | 1 |
| is_capitalized | 0 |

## 5.6 MEMM vs HMM

| Aspect | HMM | MEMM |
|--------|-----|------|
| **Type** | Generative | Discriminative |
| **Models** | P(word\|tag) | P(tag\|word, features) |
| **Features** | Word only | Arbitrary overlapping |
| **Training** | Count-based | Gradient-based |
| **Inference** | Viterbi | Modified Viterbi |

---

# 6. Bidirectionality

## 6.1 Problem with Left-to-Right

HMM and MEMM only use **left context**.

**Example**: "I saw her duck"
- Without right context: "duck" could be noun or verb
- With right context: If next word is "fly" ‚Üí verb, if next is "." ‚Üí likely noun

## 6.2 Bidirectional Models

**Approach**: Use both left AND right context

**Methods**:
1. **Bi-LSTM**: Process sequence in both directions
2. **Transformers**: Self-attention sees all positions
3. **CRF layer**: Considers whole sequence

---

# 7. Neural Network Models for POS Tagging

## 7.1 Basic Neural Tagger

```
Word ‚Üí Embedding ‚Üí Feed-forward NN ‚Üí Softmax ‚Üí Tag
```

**Limitation**: Only considers current word

## 7.2 RNN-based Tagger

```
[w‚ÇÅ, w‚ÇÇ, w‚ÇÉ, ...] ‚Üí [e‚ÇÅ, e‚ÇÇ, e‚ÇÉ, ...] ‚Üí RNN ‚Üí [h‚ÇÅ, h‚ÇÇ, h‚ÇÉ, ...] ‚Üí [t‚ÇÅ, t‚ÇÇ, t‚ÇÉ, ...]
```

**Advantage**: Captures sequential context

## 7.3 Bi-LSTM Tagger

```
Forward LSTM:  ‚Üí ‚Üí ‚Üí ‚Üí
              h‚ÇÅ h‚ÇÇ h‚ÇÉ h‚ÇÑ
              ‚Üì  ‚Üì  ‚Üì  ‚Üì
Words:        w‚ÇÅ w‚ÇÇ w‚ÇÉ w‚ÇÑ
              ‚Üë  ‚Üë  ‚Üë  ‚Üë
              h‚ÇÅ h‚ÇÇ h‚ÇÉ h‚ÇÑ
Backward LSTM: ‚Üê ‚Üê ‚Üê ‚Üê

Combined: [h_forward; h_backward] ‚Üí Dense ‚Üí Softmax ‚Üí Tag
```

**Accuracy**: ~97%

## 7.4 Bi-LSTM-CRF

**Why CRF layer?**
- Ensures valid tag sequences
- Models tag transitions globally
- Better than independent softmax per position

**Architecture**:
```
Words ‚Üí Embeddings ‚Üí Bi-LSTM ‚Üí CRF ‚Üí Tags
```

**Accuracy**: ~97.5%

## 7.5 Transformer-based (BERT)

**Process**:
1. Tokenize with WordPiece
2. Pass through BERT encoder
3. Add classification head
4. Fine-tune on POS data

**Accuracy**: ~98-99%

---

# 8. Comparison of POS Tagging Methods

| Method | Accuracy | Features | Context |
|--------|----------|----------|---------|
| **Rule-based** | ~90% | Rules | Limited |
| **HMM** | ~95% | Word only | Left (n-gram) |
| **MEMM** | ~96% | Rich features | Left |
| **Bi-LSTM** | ~97% | Embeddings | Bidirectional |
| **Bi-LSTM-CRF** | ~97.5% | Embeddings + CRF | Bidirectional |
| **BERT** | ~98.5% | Contextualized | Full sentence |

---

# 9. Handling Unknown Words

## 9.1 The OOV Problem
Words not seen in training have no emission probabilities.

## 9.2 Solutions

| Solution | Description |
|----------|-------------|
| **Suffix rules** | "-ing" ‚Üí often VBG |
| **Word shape** | "McDonals" ‚Üí NNP |
| **Subword embeddings** | FastText, BPE |
| **Character-level models** | Char-CNN, Char-LSTM |

---

# 10. Key Formulas

| Algorithm | Key Formula |
|-----------|-------------|
| Viterbi Init | V‚ÇÅ(j) = œÄ(j) √ó B(j, o‚ÇÅ) |
| Viterbi Recursion | V‚Çú(j) = max[V‚Çú‚Çã‚ÇÅ(i) √ó A(i,j)] √ó B(j, o‚Çú) |
| Log Viterbi | log V‚Çú = max[log V‚Çú‚Çã‚ÇÅ + log A] + log B |
| MaxEnt | P(y\|x) = exp(Œ£wf) / Z |
| Forward | Œ±‚Çú(j) = [Œ£·µ¢ Œ±‚Çú‚Çã‚ÇÅ(i) √ó A(i,j)] √ó B(j, o‚Çú) |

---

# üìù Practice Questions

## Q1. Viterbi Calculation
Given:
- Start: œÄ(N)=0.5, œÄ(V)=0.5
- Trans: P(V|N)=0.5, P(N|V)=0.3
- Emit: P("fish"|N)=0.2, P("fish"|V)=0.3

Fill Viterbi table for "fish fish".

## Q2. Why does MEMM use P(tag|word) instead of P(word|tag)?

## Q3. What features would you use for word "unhappily" preceded by "was"?

## Q4. Compare Bi-LSTM and Bi-LSTM-CRF. Why add CRF?

## Q5. Calculate log Viterbi score:
- log V‚ÇÅ(N) = -2
- log A(N,V) = -0.5
- log B(V, word) = -1.2

---

*Reference: Session 7 - Statistical, ML and Neural Models of POS Tagging*
