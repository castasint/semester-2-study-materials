# üìá NLP MIDSEM FLASHCARDS
## Quick Reference for Last 30 Minutes

---

# üî¥ QUESTION 1: INTRODUCTION (4 Marks)

## 4 NLP Applications
1. **Machine Translation** - Google Translate
2. **Sentiment Analysis** - Positive/Negative detection
3. **NER** - Finding names, places, organizations
4. **Question Answering** - Siri, Alexa

## 6 Levels (Bottom to Top)
**M**orphological ‚Üí **L**exical ‚Üí **S**yntactic ‚Üí **S**emantic ‚Üí **D**iscourse ‚Üí **P**ragmatic

## 3 Types of Ambiguity
- **Structural**: "I saw man with telescope" (who has it?)
- **Lexical**: "bank" (river/financial)
- **Grammatical**: "can" (modal/verb/noun)

---

# üü† QUESTION 2: N-GRAM & PERPLEXITY (4 Marks)

## Bigram
```
P(word | prev) = C(prev, word) / C(prev)
```

## Laplace Smoothing
```
P = (C + 1) / (N + V)     V = vocab size
```

## Perplexity
```
PP = P(sentence)^(-1/N)   Lower = Better!
```

**Example**: P = 0.04, N = 3 ‚Üí PP = (25)^(1/3) ‚âà 2.92

---

# üü° QUESTION 3: NEURAL LM & LLM (4 Marks)

## N-gram vs Neural LM
| N-gram | Neural |
|--------|--------|
| Fixed context | Long context |
| No generalization | Embeddings help |
| Sparsity problem | No sparsity |

## Prompting Types
- **Zero-shot**: No examples
- **One-shot**: 1 example
- **Few-shot**: Multiple examples
- **Chain-of-Thought**: Step by step reasoning

---

# üü¢ QUESTION 4: VECTOR SEMANTICS (4 Marks)

## TF-IDF
```
TF = 1 + log‚ÇÅ‚ÇÄ(count)      ‚Üê Don't forget the 1!
IDF = log‚ÇÅ‚ÇÄ(N / df)
TF-IDF = TF √ó IDF
```

## Cosine Similarity
```
cos = (A¬∑B) / (||A|| √ó ||B||)
||A|| = ‚àö(a‚ÇÅ¬≤ + a‚ÇÇ¬≤ + ...)
```

**Example**: A = [2,1,0,2], B = [1,1,2,1]
- Dot = 5, ||A|| = 3, ||B|| = ‚àö7 = 2.65
- cos = 5/(3√ó2.65) = 0.63

---

# üîµ QUESTION 5: WORD EMBEDDINGS (5 Marks)

## Word Analogy
```
v_Queen = v_King - v_Man + v_Woman
```

## Skip-gram vs CBOW
- **Skip-gram**: target ‚Üí context (better for rare)
- **CBOW**: context ‚Üí target (faster)

## Word2Vec Update
```
Error = œÉ(v¬∑u) - y    (y=1 positive, y=0 negative)
v_new = v_old - Œ∑ √ó Error √ó u
```
- Positive pair ‚Üí vectors CLOSER
- Negative pair ‚Üí vectors APART

---

# üü£ QUESTION 6: HMM POS TAGGING (4 Marks)

## HMM Components
- Hidden = Tags (NN, VB, DT)
- Observed = Words
- Transition = P(tag | prev_tag)
- Emission = P(word | tag)

## HMM Disambiguation
```
Score = P(tag | prev) √ó P(word | tag)
Choose HIGHEST score!
```

**Example**: "flies" after NN
- Score(NN) = 0.3 √ó 0.02 = 0.006
- Score(VBZ) = 0.4 √ó 0.05 = 0.020 ‚Üê Winner!

---

# ‚ö´ QUESTION 7: VITERBI & MEMM (5 Marks)

## Viterbi 3 Steps
1. **INIT**: V‚ÇÅ(j) = œÄ(j) √ó P(word‚ÇÅ|j)
2. **RECURSE**: V‚Çú(j) = max[V‚Çú‚Çã‚ÇÅ(i) √ó A(i,j)] √ó B(j,word‚Çú)
3. **BACKTRACK**: Follow pointers from max final

## HMM vs MEMM
| HMM | MEMM |
|-----|------|
| Generative | Discriminative |
| P(word\|tag) | P(tag\|word, features) |
| Limited features | Rich features |

---

# ‚ö†Ô∏è DON'T FORGET!

| Formula | Key Point |
|---------|-----------|
| TF | **1** + log‚ÇÅ‚ÇÄ(count) |
| PP | Power is **NEGATIVE**: -1/N |
| HMM | Transition **√ó** Emission |
| Viterbi | Don't forget **emission** at end |
| Cosine | Calculate **BOTH** magnitudes |

---

# üèÜ EXAM ORDER
1. Q4+Q5 (Vector + Embedding) - 9 marks
2. Q6+Q7 (HMM + Viterbi) - 9 marks
3. Q2 (N-gram) - 4 marks
4. Q3 (LLM) - 4 marks
5. Q1 (Theory) - 4 marks

---

**üçÄ Show ALL steps = Partial credit!**
