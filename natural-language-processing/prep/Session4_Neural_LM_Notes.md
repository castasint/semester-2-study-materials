# Session 4: Neural Networks and Neural Language Modeling
## AIMLCZG530 - Natural Language Processing

---

# 1. Why Neural Language Models?

## 1.1 Limitations of N-gram Models

| Limitation | Description |
|------------|-------------|
| **No generalization** | "cat" and "dog" are unrelated |
| **Sparsity** | Most n-grams never seen |
| **Fixed context** | Only n-1 words of history |
| **No semantics** | Words are just symbols |

## 1.2 Neural LM Advantages

| Advantage | Description |
|-----------|-------------|
| **Word embeddings** | Similar words have similar vectors |
| **Generalization** | "cat sat" â†’ can predict "dog sat" |
| **Long context** | RNN/Transformer can use more history |
| **Shared parameters** | Efficient representation |

---

# 2. Feed-Forward Neural Networks

## 2.1 Basic Architecture

```
Input Layer â†’ Hidden Layer(s) â†’ Output Layer
    x            h = f(Wx + b)      y = softmax(Vh)
```

## 2.2 Components

### Neurons
- Receive weighted inputs
- Apply activation function
- Produce output

### Layers
| Layer | Function |
|-------|----------|
| **Input** | Receives features |
| **Hidden** | Learns representations |
| **Output** | Produces predictions |

### Activation Functions

| Function | Formula | Properties |
|----------|---------|------------|
| **Sigmoid** | Ïƒ(x) = 1/(1+e^-x) | Output: (0,1) |
| **Tanh** | tanh(x) = (e^x - e^-x)/(e^x + e^-x) | Output: (-1,1) |
| **ReLU** | max(0, x) | Simple, effective |
| **Softmax** | e^xáµ¢ / Î£e^xâ±¼ | Probability distribution |

## 2.3 Forward Propagation

**Step-by-step for single hidden layer**:

```
1. Input: x (n-dimensional)
2. Hidden: h = Ïƒ(Wâ‚x + bâ‚)
3. Output: y = softmax(Wâ‚‚h + bâ‚‚)
```

## 2.4 Loss Functions

### Cross-Entropy Loss (for classification)
```
L = -Î£ yáµ¢ log(Å·áµ¢)
```

### Negative Log-Likelihood (for LM)
```
L = -log P(wâ‚œ | context)
```

---

# 3. Neural Language Model Architecture

## 3.1 Basic Feed-Forward LM

**Proposed by Bengio et al. (2003)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Softmax                     â”‚ â†’ P(next word)
â”‚                 (V units)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Hidden Layer                  â”‚
â”‚                 (h units)                    â”‚
â”‚              h = tanh(Wx + b)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Concatenated Embeddings            â”‚
â”‚              [eâ‚; eâ‚‚; eâ‚ƒ; eâ‚„]               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Lookup Table (E)                â”‚
â”‚           (Embedding matrix VÃ—d)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Input: Context words              â”‚
â”‚           (one-hot or indices)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.2 Architecture Details

### Input Representation
- **Context window**: n-1 previous words
- **One-hot vectors**: V-dimensional (vocabulary size)

### Embedding Layer
- **Lookup table**: E (V Ã— d matrix)
- **Word â†’ embedding**: eáµ¢ = E[wáµ¢]
- **Concatenation**: x = [eâ‚; eâ‚‚; ...; eâ‚™â‚‹â‚]

### Hidden Layer
```
h = tanh(W Â· x + b)
```
- W: Weight matrix (h Ã— (n-1)Ã—d)
- b: Bias vector
- h: Hidden units (typically 50-200)

### Output Layer
```
y = softmax(V Â· h + c)
```
- V: Output weight matrix (|V| Ã— h)
- Softmax: Converts to probability distribution

## 3.3 Mathematical Formulation

**Complete model**:
```
P(wâ‚œ | wâ‚œâ‚‹â‚, wâ‚œâ‚‹â‚‚, ..., wâ‚œâ‚‹â‚™â‚Šâ‚) = softmax(V Â· tanh(W Â· [eâ‚;...;eâ‚™â‚‹â‚] + b) + c)
```

---

# 4. Training Neural Language Models

## 4.1 Training Objective

**Maximize log-likelihood**:
```
J = Î£â‚œ log P(wâ‚œ | wâ‚œâ‚‹â‚, ..., wâ‚œâ‚‹â‚™â‚Šâ‚)
```

**Equivalently, minimize cross-entropy loss**:
```
L = -Î£â‚œ log P(wâ‚œ | context)
```

## 4.2 Backpropagation

**Compute gradients**:
1. Forward pass: compute predictions
2. Compute loss
3. Backward pass: compute gradients âˆ‚L/âˆ‚Î¸
4. Update parameters: Î¸ = Î¸ - Î·âˆ‡L

## 4.3 Stochastic Gradient Descent (SGD)

```
For each mini-batch:
    1. Forward pass
    2. Compute loss
    3. Compute gradients
    4. Update: Î¸ = Î¸ - Î·Â·âˆ‡L
```

## 4.4 Embedding Learning

**Two strategies**:

| Strategy | Description | Pros/Cons |
|----------|-------------|-----------|
| **Pre-trained** | Use Word2Vec, GloVe | Less data needed |
| **Joint training** | Learn with LM | Task-specific |
| **Frozen** | Don't update embeddings | Faster, less flexible |
| **Fine-tuned** | Update embeddings | Better, more parameters |

---

# 5. Why Neural LMs Work Better

## 5.1 Semantic Generalization

**N-gram model**:
- Seen: "The cat sat on the mat"
- Unseen: "The dog sat on the rug"
- P(sat | dog) = 0 (never seen!)

**Neural model**:
- v_cat â‰ˆ v_dog (similar embeddings)
- Can generalize: "dog sat" is probable too!

## 5.2 Shared Representations

```
Similar contexts â†’ Similar embeddings â†’ Similar predictions
```

## 5.3 Smooth Probability Estimates

- No zero probabilities (smoothing built-in)
- Continuous output space
- Gradual transitions

---

# 6. Advanced Architectures (Brief Overview)

## 6.1 Recurrent Neural Networks (RNN)

```
hâ‚œ = tanh(W_hhÂ·hâ‚œâ‚‹â‚ + W_xhÂ·xâ‚œ + b)
yâ‚œ = softmax(W_hyÂ·hâ‚œ)
```

**Advantage**: Variable-length context

## 6.2 LSTM (Long Short-Term Memory)

- Addresses vanishing gradient
- Gates: forget, input, output
- Cell state for long-term memory

## 6.3 Transformer

- Self-attention mechanism
- Parallel processing
- Foundation for BERT, GPT

---

# 7. Comparison: N-gram vs Neural LM

| Aspect | N-gram LM | Neural LM |
|--------|-----------|-----------|
| Context | Fixed (n-1) | Variable |
| Parameters | O(V^n) | O(VÃ—d + dÂ²) |
| Sparsity | Major issue | Not an issue |
| Training | Counting | Gradient descent |
| Generalization | None | Embedding-based |
| Interpretability | High | Low |
| Computation | Fast lookup | Matrix multiply |
| Memory | Large tables | Network weights |

---

# 8. Key Equations

| Concept | Equation |
|---------|----------|
| Hidden layer | h = tanh(Wx + b) |
| Output | y = softmax(Vh + c) |
| Softmax | P(i) = e^záµ¢ / Î£e^zâ±¼ |
| Cross-entropy | L = -Î£ yáµ¢ log(Å·áµ¢) |
| SGD update | Î¸ = Î¸ - Î·âˆ‡L |

---

# ğŸ“ Practice Questions

## Q1. Compare N-gram and Neural Language Models on:
a) Handling unseen word combinations
b) Memory requirements
c) Training complexity

## Q2. For a 4-gram neural LM with:
- Vocabulary size V = 10,000
- Embedding dimension d = 100
- Hidden layer size h = 200

How many parameters in the embedding layer?

## Q3. Why does a neural LM generalize better than an N-gram model?

## Q4. Explain the role of the softmax function in neural language models.

## Q5. What is the advantage of using pre-trained embeddings vs learning them jointly?

---

*Reference: Session 4 - Neural Networks and Neural Language Modeling*
