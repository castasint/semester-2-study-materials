# Session 3: N-gram Language Modeling
## AIMLCZG530 - Natural Language Processing

---

# 1. Language Models - Introduction

## 1.1 What is a Language Model?

A **Language Model** assigns probabilities to sequences of words.

**Two key tasks**:
1. **P(W)**: Probability of a sentence/sequence
2. **P(w‚Çô | w‚ÇÅ...w‚Çô‚Çã‚ÇÅ)**: Probability of next word given history

## 1.2 Why Language Models Matter

| Application | How LM Helps |
|-------------|--------------|
| **Speech Recognition** | Choose most likely word sequence |
| **Machine Translation** | Select fluent output |
| **Spelling Correction** | Rank correction candidates |
| **Text Generation** | Predict next words |
| **Autocomplete** | Suggest completions |

## 1.3 Formal Definition

For a sentence W = w‚ÇÅ, w‚ÇÇ, ..., w‚Çô:
```
P(W) = P(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)
```

---

# 2. N-Grams

## 2.1 Chain Rule of Probability

**Exact computation**:
```
P(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô) = P(w‚ÇÅ) √ó P(w‚ÇÇ|w‚ÇÅ) √ó P(w‚ÇÉ|w‚ÇÅ,w‚ÇÇ) √ó ... √ó P(w‚Çô|w‚ÇÅ...w‚Çô‚Çã‚ÇÅ)
```

**Problem**: Need to estimate P(w‚Çô | w‚ÇÅ...w‚Çô‚Çã‚ÇÅ) - history can be very long!

## 2.2 Markov Assumption

**Key Insight**: Approximate by looking at only last (n-1) words

```
P(w‚Çô | w‚ÇÅ...w‚Çô‚Çã‚ÇÅ) ‚âà P(w‚Çô | w‚Çô‚Çã‚Çñ‚Çä‚ÇÅ...w‚Çô‚Çã‚ÇÅ)
```

## 2.3 Types of N-grams

| N | Name | Formula | Context |
|---|------|---------|---------|
| 1 | Unigram | P(w‚Çô) | No context |
| 2 | Bigram | P(w‚Çô \| w‚Çô‚Çã‚ÇÅ) | 1 previous word |
| 3 | Trigram | P(w‚Çô \| w‚Çô‚Çã‚ÇÇ, w‚Çô‚Çã‚ÇÅ) | 2 previous words |
| 4 | 4-gram | P(w‚Çô \| w‚Çô‚Çã‚ÇÉ, w‚Çô‚Çã‚ÇÇ, w‚Çô‚Çã‚ÇÅ) | 3 previous words |

## 2.4 Estimating N-gram Probabilities (MLE)

**Maximum Likelihood Estimation**:

### Bigram
```
P(w‚Çô | w‚Çô‚Çã‚ÇÅ) = Count(w‚Çô‚Çã‚ÇÅ, w‚Çô) / Count(w‚Çô‚Çã‚ÇÅ)
```

### Trigram
```
P(w‚Çô | w‚Çô‚Çã‚ÇÇ, w‚Çô‚Çã‚ÇÅ) = Count(w‚Çô‚Çã‚ÇÇ, w‚Çô‚Çã‚ÇÅ, w‚Çô) / Count(w‚Çô‚Çã‚ÇÇ, w‚Çô‚Çã‚ÇÅ)
```

## 2.5 Complete Example

**Corpus**:
```
<s> I am happy </s>
<s> I am learning NLP </s>
<s> I love NLP </s>
```

**Counts**:
- Count("I") = 3
- Count("I", "am") = 2
- Count("I", "love") = 1
- Count("am") = 2
- Count("am", "happy") = 1
- Count("am", "learning") = 1

**Bigram Probabilities**:
```
P(am | I) = Count(I, am) / Count(I) = 2/3 = 0.667
P(love | I) = Count(I, love) / Count(I) = 1/3 = 0.333
P(happy | am) = Count(am, happy) / Count(am) = 1/2 = 0.5
P(learning | am) = Count(am, learning) / Count(am) = 1/2 = 0.5
```

## 2.6 Sentence Probability

**Example**: P("I am happy")

```
P(<s> I am happy </s>) = P(I|<s>) √ó P(am|I) √ó P(happy|am) √ó P(</s>|happy)
```

---

# 3. Generalization and the Zero Problem

## 3.1 The Sparsity Problem

**Training corpus**: "I saw a dog"
**Test sentence**: "I saw a cat"

P(cat | a) = Count(a, cat) / Count(a) = 0/1 = **0**

**Result**: Entire sentence probability becomes 0!

## 3.2 Why This Happens

- Language is **creative** - infinite possible sentences
- Any finite corpus will miss many valid n-grams
- **Unseen n-grams** get probability 0

## 3.3 Solutions Overview

| Technique | Approach |
|-----------|----------|
| **Smoothing** | Add counts to unseen events |
| **Backoff** | Use lower-order n-gram if higher-order unseen |
| **Interpolation** | Combine multiple n-gram orders |

---

# 4. Smoothing Techniques

## 4.1 Laplace (Add-1) Smoothing

**Idea**: Add 1 to all counts

**Formula**:
```
P_Laplace(w‚Çô | w‚Çô‚Çã‚ÇÅ) = [Count(w‚Çô‚Çã‚ÇÅ, w‚Çô) + 1] / [Count(w‚Çô‚Çã‚ÇÅ) + V]
```

Where V = vocabulary size

**Example**:
- Count("dog", "runs") = 0
- Count("dog") = 100
- V = 10,000

```
P_MLE(runs | dog) = 0/100 = 0

P_Laplace(runs | dog) = (0 + 1) / (100 + 10,000) = 1/10,100 ‚âà 0.0001
```

**Problem**: Steals too much probability from seen events

## 4.2 Add-k Smoothing

**Generalization**: Add k instead of 1 (k < 1)

```
P_Add-k(w‚Çô | w‚Çô‚Çã‚ÇÅ) = [Count(w‚Çô‚Çã‚ÇÅ, w‚Çô) + k] / [Count(w‚Çô‚Çã‚ÇÅ) + k√óV]
```

## 4.3 Linear Interpolation

**Idea**: Combine unigram, bigram, trigram with weights

**Formula**:
```
P(w‚Çô|w‚Çô‚Çã‚ÇÇ,w‚Çô‚Çã‚ÇÅ) = Œª‚ÇÅ√óP(w‚Çô|w‚Çô‚Çã‚ÇÇ,w‚Çô‚Çã‚ÇÅ) + Œª‚ÇÇ√óP(w‚Çô|w‚Çô‚Çã‚ÇÅ) + Œª‚ÇÉ√óP(w‚Çô)
```

**Constraints**: Œª‚ÇÅ + Œª‚ÇÇ + Œª‚ÇÉ = 1

**Example**:
- Œª‚ÇÅ = 0.6, Œª‚ÇÇ = 0.3, Œª‚ÇÉ = 0.1
- P(runs | dog, the) = 0.01 (trigram)
- P(runs | the) = 0.05 (bigram)
- P(runs) = 0.001 (unigram)

```
P_interp = 0.6√ó0.01 + 0.3√ó0.05 + 0.1√ó0.001
         = 0.006 + 0.015 + 0.0001
         = 0.0211
```

## 4.4 Backoff

**Idea**: Use higher-order n-gram if available, otherwise "back off" to lower order

**Algorithm**:
```
if Count(w‚Çô‚Çã‚ÇÇ, w‚Çô‚Çã‚ÇÅ, w‚Çô) > 0:
    use P_trigram(w‚Çô | w‚Çô‚Çã‚ÇÇ, w‚Çô‚Çã‚ÇÅ)
elif Count(w‚Çô‚Çã‚ÇÅ, w‚Çô) > 0:
    use Œ± √ó P_bigram(w‚Çô | w‚Çô‚Çã‚ÇÅ)
else:
    use Œ± √ó Œ± √ó P_unigram(w‚Çô)
```

Where Œ± is a discount factor

---

# 5. Stupid Backoff

## 5.1 Motivation
- Used at **web scale** (billions of words)
- Google's solution for very large corpora
- Simple and effective

## 5.2 Formula

```
S(w·µ¢ | w·µ¢‚Çã‚Çñ‚Çä‚ÇÅ...w·µ¢‚Çã‚ÇÅ) = 
    Count(w·µ¢‚Çã‚Çñ‚Çä‚ÇÅ...w·µ¢) / Count(w·µ¢‚Çã‚Çñ‚Çä‚ÇÅ...w·µ¢‚Çã‚ÇÅ)    if count > 0
    0.4 √ó S(w·µ¢ | w·µ¢‚Çã‚Çñ‚Çä‚ÇÇ...w·µ¢‚Çã‚ÇÅ)                   otherwise
```

## 5.3 Key Properties

| Property | Description |
|----------|-------------|
| **No normalization** | Not a proper probability distribution |
| **Works well at scale** | Effective for large datasets |
| **Simple** | Easy to implement |
| **Fixed backoff weight** | Always multiply by 0.4 |

## 5.4 Example

Calculate S(happy | am, I):

```
If Count(I, am, happy) > 0:
    S = Count(I, am, happy) / Count(I, am)
Else:
    S = 0.4 √ó S(happy | am)
    
    If Count(am, happy) > 0:
        S = 0.4 √ó [Count(am, happy) / Count(am)]
    Else:
        S = 0.4 √ó 0.4 √ó S(happy)
        S = 0.16 √ó [Count(happy) / Total_words]
```

---

# 6. Evaluating Language Models

## 6.1 Extrinsic Evaluation
- Evaluate on downstream task (MT, ASR)
- **Pros**: Real-world performance
- **Cons**: Expensive, task-specific

## 6.2 Intrinsic Evaluation: Perplexity

### Definition
Perplexity measures how "surprised" the model is by test data.

### Formula
```
PP(W) = P(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)^(-1/N)
      = ‚Åø‚àö(1 / P(W))
```

For bigram model:
```
PP(W) = [‚àè·µ¢ P(w·µ¢ | w·µ¢‚Çã‚ÇÅ)]^(-1/N)
```

### Interpretation

| Perplexity | Interpretation |
|------------|----------------|
| Lower | Better model |
| Higher | Worse model |
| = k | Like choosing uniformly from k words |

### Intuition
- PP = 10 means model is as confused as randomly picking from 10 words
- "Branching factor" of the model

## 6.3 Perplexity Calculation Example

**Sentence**: "I love NLP" (N = 3 words)

**Probabilities**:
- P(I) = 0.1
- P(love | I) = 0.2
- P(NLP | love) = 0.05

**Calculate**:
```
P(sentence) = 0.1 √ó 0.2 √ó 0.05 = 0.001

PP = (0.001)^(-1/3)
   = (1/0.001)^(1/3)
   = (1000)^(1/3)
   = 10
```

Perplexity = 10 ‚Üí Model choosing from ~10 equally likely words

## 6.4 Log Perplexity

To avoid numerical underflow:
```
log PP = -1/N √ó Œ£·µ¢ log P(w·µ¢ | history)
```

## 6.5 Typical Perplexity Values

| Model | Perplexity (WSJ corpus) |
|-------|-------------------------|
| Unigram | ~962 |
| Bigram | ~170 |
| Trigram | ~109 |
| Neural LM | ~50-80 |

---

# 7. Practical Considerations

## 7.1 Unknown Words (OOV)

**Problem**: Words not in vocabulary

**Solutions**:
1. **<UNK> token**: Replace rare words with special token
2. **Threshold**: Words appearing < k times ‚Üí <UNK>
3. **Character-level**: Handle at character level

## 7.2 Sentence Boundaries

Add special tokens:
- **<s>**: Start of sentence
- **</s>**: End of sentence

**Example**:
```
"I love NLP" ‚Üí "<s> I love NLP </s>"
```

## 7.3 Log Probabilities

**Problem**: Probability products become very small

**Solution**: Work in log space
```
log P(w‚ÇÅ...w‚Çô) = Œ£·µ¢ log P(w·µ¢ | history)
```

## 7.4 Choosing N

| N | Pros | Cons |
|---|------|------|
| 1 (Unigram) | Many counts | No context |
| 2 (Bigram) | Some context | Limited history |
| 3 (Trigram) | Better context | Sparse counts |
| 4+ | More context | Very sparse |

**Trade-off**: More context vs. more reliable estimates

---

# 8. N-gram vs Neural Language Models

| Aspect | N-gram | Neural LM |
|--------|--------|-----------|
| **Context** | Fixed (n-1 words) | Variable/Long |
| **Parameters** | O(V^n) | O(V √ó d) |
| **Generalization** | None (exact match) | Embedding-based |
| **Training** | Counting | Gradient descent |
| **Sparsity** | Major problem | Not an issue |
| **Interpretability** | High | Low |
| **Speed** | Fast | Slower |

---

# 9. Key Formulas Summary

| Concept | Formula |
|---------|---------|
| Bigram MLE | `P(w‚Çô\|w‚Çô‚Çã‚ÇÅ) = C(w‚Çô‚Çã‚ÇÅ,w‚Çô) / C(w‚Çô‚Çã‚ÇÅ)` |
| Laplace | `P = (C+1) / (N+V)` |
| Interpolation | `P = Œª‚ÇÅP‚ÇÉ + Œª‚ÇÇP‚ÇÇ + Œª‚ÇÉP‚ÇÅ` |
| Stupid Backoff | `S = 0.4 √ó S(lower-order)` |
| Perplexity | `PP = P(W)^(-1/N)` |
| Log Perplexity | `log PP = -1/N √ó Œ£ log P(w·µ¢)` |

---

# üìù Practice Questions

## Q1. Bigram Probability
Corpus: "I am happy. I am sad. I love happy."
Calculate: P(happy | am), P(sad | am), P(am | I)

## Q2. Laplace Smoothing
- Count("the", "cat") = 5
- Count("the") = 100
- V = 20,000
Calculate P_Laplace(cat | the)

## Q3. Perplexity
Sentence: "dogs run fast" (3 words)
P(dogs) = 0.01, P(run|dogs) = 0.1, P(fast|run) = 0.05
Calculate perplexity.

## Q4. Linear Interpolation
Œª‚ÇÅ=0.5, Œª‚ÇÇ=0.3, Œª‚ÇÉ=0.2
P(runs|dog,the) = 0.02, P(runs|the) = 0.04, P(runs) = 0.001
Calculate interpolated probability.

## Q5. Stupid Backoff
Count(I, love, NLP) = 0
Count(love, NLP) = 10
Count(love) = 200
Calculate S(NLP | love, I)

---

*Reference: Session 3 - N-gram Language Modeling*
